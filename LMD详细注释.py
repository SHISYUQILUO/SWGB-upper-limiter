import os
import numpy as np
import matplotlib.pyplot as plt
import torch
from scipy.stats import kurtosis, pearsonr
from sbi.inference import SNPE, simulate_for_sbi
from sbi.utils import BoxUniform
from tqdm import tqdm
import glob
from collections import defaultdict
import torch.nn.functional as F
import datetime  # ç§»åŠ¨åˆ°é¡¶éƒ¨ç»Ÿä¸€å¯¼å…¥

# ==========================================
# 1. GPU è®¾ç½®ä¸ä¼˜åŒ–
# ==========================================
"""
ã€æ–¹æ³•è®ºã€‘GPUåŠ é€Ÿè®¡ç®—åœ¨GWæ•°æ®åˆ†æä¸­çš„å¿…è¦æ€§

GWæ•°æ®å…·æœ‰é«˜é‡‡æ ·ç‡ï¼ˆé€šå¸¸4096Hzæˆ–æ›´é«˜ï¼‰å’Œé•¿æŒç»­æ—¶é—´ï¼ˆæ•°å°æ—¶è‡³æ•°å¹´ï¼‰çš„ç‰¹ç‚¹ï¼Œ
äº§ç”ŸTBçº§æ•°æ®é‡ã€‚ä¼ ç»ŸCPUè®¡ç®—åœ¨å¤„ç†å¤§è§„æ¨¡æ¨¡æ‹Ÿå’Œç¥ç»ç½‘ç»œè®­ç»ƒæ—¶æ•ˆç‡æä½ã€‚
æœ¬ä»£ç é‡‡ç”¨NVIDIA GPUåŠ é€Ÿï¼Œåˆ©ç”¨CUDAæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œè®¡ç®—ã€‚

ã€ç‰©ç†èƒŒæ™¯ã€‘SGWBä¿¡å·æå…¶å¾®å¼±ï¼ˆh~10^-23ï¼‰ï¼Œéœ€è¦æµ·é‡è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¥ä¼°è®¡
æ£€æµ‹æé™ï¼Œè¿™è¦æ±‚è®¡ç®—æ¶æ„å¿…é¡»å…·å¤‡é«˜ååé‡ã€‚

ã€æ›¿ä»£æ–¹æ¡ˆå¯¹æ¯”ã€‘
- CPUå¹¶è¡Œï¼ˆOpenMP/MPIï¼‰ï¼šé€‚ç”¨äºå°è§„æ¨¡é›†ç¾¤ï¼Œä½†å•èŠ‚ç‚¹æ€§èƒ½å—é™
- TPUï¼šé€‚åˆå¤§è§„æ¨¡çŸ©é˜µè¿ç®—ï¼Œä½†ç”Ÿæ€æ”¯æŒä¸å¦‚CUDAå®Œå–„ï¼Œä¸”GWé¢†åŸŸä»£ç å¤šä¸ºCUDAä¼˜åŒ–
- FPGAï¼šæä½å»¶è¿Ÿï¼Œä½†å¼€å‘å‘¨æœŸé•¿ï¼Œçµæ´»æ€§å·®

ã€é€‰æ‹©ç†ç”±ã€‘PyTorchçš„CUDAåç«¯æä¾›è‡ªåŠ¨å¾®åˆ†å’ŒåŠ¨æ€å›¾ï¼Œå®Œç¾é€‚é…SBIï¼ˆSimulation-Based Inferenceï¼‰
çš„è¿­ä»£è®­ç»ƒéœ€æ±‚ï¼Œä¸”GWç¤¾åŒºï¼ˆå¦‚PyCBCã€Bilbyï¼‰å·²å»ºç«‹å®Œæ•´çš„GPUå·¥å…·é“¾ã€‚
"""
if not torch.cuda.is_available():
    raise RuntimeError("âŒ æœªæ£€æµ‹åˆ°GPUï¼æ­¤ä»£ç éœ€è¦GPUåŠ é€Ÿ")
    
device = torch.device("cuda")
torch.backends.cudnn.benchmark = True  # è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å·ç§¯ç®—æ³•ï¼Œé€‚åˆè¾“å…¥å°ºå¯¸å›ºå®šçš„åœºæ™¯
torch.backends.cuda.matmul.allow_tf32 = True  # TensorFloat-32æ··åˆç²¾åº¦ï¼Œä¿æŒç²¾åº¦åŒæ—¶æå‡40%é€Ÿåº¦
torch.backends.cudnn.allow_tf32 = True

print(f"ğŸš€ ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

torch.cuda.empty_cache()

# ==========================================
# 2. è·¯å¾„è®¾ç½®
# ==========================================
"""
ã€æ–¹æ³•è®ºã€‘å®éªŒç®¡ç†çš„å¯é‡å¤æ€§åŸåˆ™

GWç ”ç©¶æ¶‰åŠå¤æ‚çš„æ•°æ®å¤„ç†æµç¨‹ï¼Œå¿…é¡»ç¡®ä¿æ¯æ¬¡å®éªŒçš„å¯è¿½æº¯æ€§ã€‚
ä½¿ç”¨æ—¶é—´æˆ³å‘½åè¾“å‡ºç›®å½•æ˜¯ç‰©ç†å®éªŒçš„æ ‡å‡†å®è·µï¼Œç¬¦åˆLIGO/Virgoçš„æ•°æ®ç®¡ç†è§„èŒƒã€‚

ã€ç ”ç©¶ä¸»é¢˜å¥‘åˆåº¦ã€‘SGWBéé«˜æ–¯æ€§ç ”ç©¶éœ€è¦ä¸¥æ ¼åŒºåˆ†ä¸åŒè§‚æµ‹å‘¨æœŸï¼ˆO3a/O3b/O4aï¼‰çš„æ•°æ®ï¼Œ
é˜²æ­¢ä¸åŒçµæ•åº¦æ—¶æœŸçš„å™ªå£°ç‰¹æ€§æ··æ·†ï¼Œè¿™æ˜¯æ¢æµ‹ Burst-like SGWB çš„å…³é”®è´¨é‡æ§åˆ¶ã€‚
"""
DATA_DIR = r"C:\Users\20466\Desktop\æ•°æ®"

timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
OUTPUT_DIR = os.path.join(DATA_DIR, f"output_{timestamp}")
CACHE_DIR = os.path.join(OUTPUT_DIR, "cache")
MODEL_DIR = os.path.join(OUTPUT_DIR, "models")

os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

SAVE_FILE = os.path.join(CACHE_DIR, "sensitivity_curves_gpu.pt")
print(f"ğŸ“ æ–°å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

# ==========================================
# 3. GPUåŠ é€Ÿçš„æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ==========================================
def discover_data_groups(data_dir):
    """
    ã€æ–¹æ³•è®ºã€‘è‡ªåŠ¨åŒ–æ•°æ®å‘ç°ä¸é…å¯¹ç­–ç•¥
    
    LIGOæ•°æ®ä»¥å¸§æ–‡ä»¶ï¼ˆframe filesï¼‰å½¢å¼å­˜å‚¨ï¼ŒæŒ‰æ¢æµ‹å™¨ï¼ˆH1/L1ï¼‰å’Œæ—¶é—´æ®µç»„ç»‡ã€‚
    æœ¬å‡½æ•°å®ç°å¯å‘å¼æ–‡ä»¶åè§£æï¼Œè‡ªåŠ¨åŒ¹é…åŒæ¢æµ‹å™¨æ•°æ®å¯¹ã€‚
    
    ã€ç‰©ç†æ„ä¹‰ã€‘SGWBæ¢æµ‹å¿…é¡»åˆ©ç”¨æ¢æµ‹å™¨ä¹‹é—´çš„ç›¸å…³æ€§ï¼ˆcorrelated noiseï¼‰ï¼Œ
    å•æ¢æµ‹å™¨æ— æ³•åŒºåˆ†GWä¿¡å·ä¸æœ¬åœ°å™ªå£°ã€‚H1ï¼ˆHanfordï¼‰å’ŒL1ï¼ˆLivingstonï¼‰çš„
    é‡åˆåˆ†ææ˜¯æ ‡å‡†æµç¨‹ã€‚
    
    ã€ç®—æ³•è®¾è®¡ã€‘
    1. æ–‡ä»¶åè§£æï¼šdataset_detector_timestamp_suffix.pt
       - dataset: è§‚æµ‹å‘¨æœŸï¼ˆO3a/O3b/O4aï¼‰ï¼Œåæ˜ ä¸åŒçµæ•åº¦é˜¶æ®µ
       - detector: H1/L1ï¼Œç¡®ä¿ç©ºé—´å…³è”æµ‹é‡
       - timestamp: GPSæ—¶é—´æˆ–æœ¬åœ°æ—¶é—´ï¼Œæ ‡è¯†æ•°æ®æ®µ
    
    ã€æ›¿ä»£æ–¹æ¡ˆã€‘
    - ä½¿ç”¨LIGOçš„gwdatafindæœåŠ¡ç›´æ¥æŸ¥è¯¢æ•°æ®ä¸­å¿ƒï¼šé€‚åˆç”Ÿäº§ç¯å¢ƒï¼Œä½†éœ€ç½‘ç»œè®¿é—®
    - æ‰‹åŠ¨CSVç´¢å¼•ï¼šé€‚åˆå°è§„æ¨¡æµ‹è¯•ï¼Œä½†ä¸å…·å¤‡æ‰©å±•æ€§
    
    ã€å½“å‰æ–¹æ¡ˆä¼˜åŠ¿ã€‘ç¦»çº¿å¤„ç†LIGOå…¬å¼€æ•°æ®ï¼ˆGWOSCæ ¼å¼ï¼‰ï¼Œæ— éœ€è®¤è¯ï¼Œ
    é€‚åˆæ–¹æ³•å­¦å¼€å‘å’Œç®—æ³•éªŒè¯é˜¶æ®µã€‚
    """
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
    
    pt_files = glob.glob(os.path.join(data_dir, "*.pt"))
    print(f"ğŸ“‚ å‘ç° {len(pt_files)} ä¸ª .pt æ–‡ä»¶")
    
    # ä½¿ç”¨defaultdictè‡ªåŠ¨åˆå§‹åŒ–ç©ºåˆ—è¡¨ï¼Œç®€åŒ–åˆ†ç»„é€»è¾‘
    groups = defaultdict(list)
    
    for f in pt_files:
        filename = os.path.basename(f)
        parts = filename.replace('.pt', '').split('_')
        
        if len(parts) >= 3:
            dataset = parts[0]      # è§‚æµ‹å‘¨æœŸï¼šO3aï¼ˆ2019.4-2019.9ï¼‰ã€O3bï¼ˆ2019.11-2020.3ï¼‰ã€O4aï¼ˆ2023.5-2024.1ï¼‰
            detector = parts[1]     # æ¢æµ‹å™¨æ ‡è¯†ï¼šH1ï¼ˆHanfordï¼‰æˆ– L1ï¼ˆLivingstonï¼‰
            timestamp = parts[2]    # æ—¶é—´æˆ³ï¼šæ ‡è¯†åŒä¸€æ—¶é—´æ®µçš„åŒæ¢æµ‹å™¨æ•°æ®
            groups[(dataset, timestamp)].append({
                'file': f,
                'detector': detector,
                'dataset': dataset,
                'timestamp': timestamp,
                'suffix': parts[3] if len(parts) > 3 else '0'
            })
    
    # å¼ºåˆ¶åŒæ¢æµ‹å™¨é…å¯¹ï¼šSGWBæ¢æµ‹çš„å¿…è¦æ¡ä»¶
    data_groups = defaultdict(list)
    for (dataset, timestamp), files in groups.items():
        h1_file = next((f['file'] for f in files if f['detector'] == 'H1'), None)
        l1_file = next((f['file'] for f in files if f['detector'] == 'L1'), None)
        
        if h1_file and l1_file:
            data_groups[dataset].append({
                'h1': h1_file,
                'l1': l1_file,
                'timestamp': timestamp,
                'name': f"{dataset}_{timestamp}"
            })
    
    print("\nğŸ“Š æ•°æ®ç»„ç»Ÿè®¡:")
    for dataset in sorted(data_groups.keys()):
        groups_list = data_groups[dataset]
        print(f"   {dataset}: {len(groups_list)} ç»„æ•°æ®")
        for g in groups_list[:2]:
            print(f"      - {g['name']}")
        if len(groups_list) > 2:
            print(f"      ... è¿˜æœ‰ {len(groups_list)-2} ç»„")
    
    return data_groups

def load_and_preprocess_to_gpu(group_info, target_fs=2048.0, seg_len=8192):
    """
    ã€æ–¹æ³•è®ºã€‘é›¶æ‹·è´ï¼ˆZero-Copyï¼‰æ•°æ®ç®¡é“
    
    ä¼ ç»Ÿæµç¨‹ï¼šç£ç›˜â†’CPUå†…å­˜â†’é¢„å¤„ç†â†’GPUæ˜¾å­˜ï¼ˆä¸¤æ¬¡å†…å­˜æ‹·è´ï¼‰
    æœ¬ä¼˜åŒ–ï¼šç£ç›˜â†’CPUå†…å­˜â†’å¼‚æ­¥ä¸Šä¼ è‡³GPUâ†’GPUé¢„å¤„ç†ï¼ˆå•æ¬¡åŒæ­¥ç‚¹ï¼‰
    
    ã€ç‰©ç†é¢„å¤„ç†ã€‘
    1. æ ‡å‡†åŒ–ï¼š(x - Î¼) / Ïƒï¼Œæ¶ˆé™¤ä¸åŒæ—¶é—´æ®µå™ªå£°æ°´å¹³çš„ç»å¯¹å·®å¼‚
    2. è£å‰ªï¼šé™åˆ¶åœ¨Â±20Ïƒï¼ŒæŠ‘åˆ¶glitchesï¼ˆç¬æ€å™ªå£° artifactï¼‰çš„æç«¯å€¼å½±å“
    
    ã€ç ”ç©¶ä¸»é¢˜å…³è”ã€‘SGWBéé«˜æ–¯æ€§ç ”ç©¶å¯¹å¼‚å¸¸å€¼æåº¦æ•æ„Ÿï¼Œå› ä¸ºglitcheså¯èƒ½è¢«è¯¯åˆ¤ä¸ºburstä¿¡å·ã€‚
    3Ïƒè£å‰ªè¿‡äºæ¿€è¿›ä¼šæŸå¤±çœŸå®çš„éé«˜æ–¯ä¿¡æ¯ï¼Œ20Ïƒæ˜¯ä¿å®ˆé€‰æ‹©ï¼ˆå‚è€ƒï¼šLIGO glitchç ”ç©¶[1]ï¼‰ã€‚
    
    ã€å‚æ•°é€‰æ‹©ã€‘
    - target_fs=2048Hzï¼šNyquisté¢‘ç‡ä¸º1024Hzï¼Œè¶³ä»¥è¦†ç›–LIGOæ•æ„Ÿé¢‘æ®µï¼ˆ10-1000Hzï¼‰
    - seg_len=8192ï¼š4ç§’æ•°æ®æ®µï¼Œå¹³è¡¡é¢‘ç‡åˆ†è¾¨ç‡ï¼ˆÎ”f=0.25Hzï¼‰ä¸å±€éƒ¨å¹³ç¨³æ€§å‡è®¾
    
    ã€æ›¿ä»£æ–¹æ¡ˆã€‘
    - åœ¨CPUè¿›è¡ŒWelchå‘¨æœŸå›¾ä¼°è®¡åä¼ GPUï¼šå‡å°‘æ•°æ®ä¼ è¾“é‡ï¼Œä½†æŸå¤±æ—¶é—´åŸŸä¿¡æ¯ï¼Œ
      ä¸é€‚åˆburstä¿¡å·åˆ†æï¼ˆéœ€è¦ç²¾ç¡®çš„æ—¶é—´é‡åˆï¼‰
    - ä½¿ç”¨DALIåº“è¿›è¡ŒGPUè§£ç ï¼šéœ€NVIDIA GPUä¸“ç”¨æ ¼å¼ï¼Œé€šç”¨æ€§ä¸è¶³
    
    [1] Allen et al., "FINDCHIRP: An algorithm for detection of gravitational waves..."
    """
    try:
        # torch.loadç›´æ¥æ˜ å°„åˆ°CPUå†…å­˜ï¼Œé¿å…GPUå†…å­˜ç¢ç‰‡
        h1_cpu = torch.load(group_info['h1'], weights_only=False)
        l1_cpu = torch.load(group_info['l1'], weights_only=False)
        
        # ç±»å‹å®‰å…¨ï¼šå¤„ç†numpyæ•°ç»„å’Œtorchå¼ é‡çš„æ··åˆè¾“å…¥
        if not isinstance(h1_cpu, torch.Tensor):
            h1_cpu = torch.tensor(h1_cpu, dtype=torch.float32)
            l1_cpu = torch.tensor(l1_cpu, dtype=torch.float32)
        
        # éé˜»å¡ä¼ è¾“ï¼ˆnon_blocking=Trueï¼‰å¯ä¸CPUè®¡ç®—é‡å ï¼Œä½†æ­¤å¤„æ•°æ®ä¾èµ–æ€§å¼º
        h1_gpu = h1_cpu.to(device).float()
        l1_gpu = l1_cpu.to(device).float()
        
        # GPUå‘é‡åŒ–æ“ä½œï¼šåˆ©ç”¨CUDA coreçš„SIMTæ¶æ„
        h1_gpu = (h1_gpu - h1_gpu.mean()) / (h1_gpu.std() + 1e-10)  # æ•°å€¼ç¨³å®šæ€§ï¼šé¿å…é™¤é›¶
        l1_gpu = (l1_gpu - l1_gpu.mean()) / (l1_gpu.std() + 1e-10)
        h1_gpu = torch.clamp(h1_gpu, -20.0, 20.0)  # Hard limitingï¼Œéçº¿æ€§å¤„ç†ä½†ä¿æŒå•è°ƒæ€§
        l1_gpu = torch.clamp(l1_gpu, -20.0, 20.0)
        
        print(f"   âœ… {group_info['name']}: å·²åŠ è½½åˆ°GPU (H1={h1_gpu.shape}, L1={l1_gpu.shape})")
        return h1_gpu, l1_gpu
        
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥ {group_info['name']}: {e}")
        return None, None

# ==========================================
# 4. GPUæ‰¹é‡æ¨¡æ‹Ÿå™¨
# ==========================================
class GPUBatchSimulator:
    """
    ã€æ ¸å¿ƒæ–¹æ³•è®ºã€‘åŸºäºSBIï¼ˆSimulation-Based Inferenceï¼‰çš„ä¼¼ç„¶æ— å…³æ¨æ–­
    
    ä¼ ç»ŸGWå‚æ•°ä¼°è®¡ï¼ˆå¦‚MCMCï¼‰éœ€è¦æ˜¾å¼ä¼¼ç„¶å‡½æ•° L(d|Î¸)ã€‚
    å¯¹äºéé«˜æ–¯SGWBï¼Œä¼¼ç„¶å‡½æ•°æ— è§£æå½¢å¼ï¼ˆæ¶‰åŠé«˜ç»´éé«˜æ–¯ç§¯åˆ†ï¼‰ã€‚
    
    SBIé€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹  p(Î¸|x) çš„åéªŒåˆ†å¸ƒï¼Œç»•è¿‡ä¼¼ç„¶è®¡ç®—ã€‚
    å…·ä½“é‡‡ç”¨SNPEï¼ˆSequential Neural Posterior Estimationï¼‰ç³»åˆ—ç®—æ³•ï¼š
    
    ã€ç®—æ³•æµç¨‹ã€‘
    1. ä»å…ˆéªŒ p(Î¸) é‡‡æ ·å‚æ•°
    2. è¿è¡Œæ¨¡æ‹Ÿå™¨ç”Ÿæˆæ•°æ® x = simulator(Î¸) + noise
    3. è®­ç»ƒç¥ç»ç½‘ç»œï¼ˆMAF: Masked Autoregressive Flowï¼‰å­¦ä¹  p(Î¸|x)
    4. è¿­ä»£ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰ï¼šä½¿ç”¨å½“å‰åéªŒä½œä¸ºæ–°å…ˆéªŒï¼Œèšç„¦é«˜ä¼¼ç„¶åŒºåŸŸ
    
    ã€ç ”ç©¶ä¸»é¢˜å¥‘åˆåº¦ã€‘
    Burst-like SGWB çš„æ³¢å½¢æœªçŸ¥ï¼ˆä»…çŸ¥ç»Ÿè®¡ç‰¹æ€§ï¼šOmega(f)å’Œå ç©ºæ¯”Î¾ï¼‰ï¼Œ
    æ— æ³•ä½¿ç”¨åŒ¹é…æ»¤æ³¢ï¼ˆmatched filteringï¼‰ã€‚SBIå¤©ç„¶é€‚åˆ"ä¼¼ç„¶è‡ªç”±"çš„æ¨æ–­é—®é¢˜ã€‚
    
    ã€ç‰©ç†æ¨¡å‹ã€‘
    ä¿¡å·æ¨¡å‹ï¼šh(t) = Î£ A_i Î´(t - t_i)ï¼Œå…¶ä¸­ï¼š
    - A_i ~ N(0, Ïƒ_A^2)ï¼šçˆ†å‘å¹…åº¦æœä»é«˜æ–¯åˆ†å¸ƒï¼ˆä¸­å¿ƒæé™å®šç†ä¸‹çš„ç®€åŒ–ï¼‰
    - t_iï¼šçˆ†å‘åˆ°è¾¾æ—¶é—´ï¼Œæ³Šæ¾è¿‡ç¨‹ï¼ˆé€Ÿç‡ Î» = Î¾ * f_sï¼ŒÎ¾ä¸ºå ç©ºæ¯”ï¼‰
    - å¹…åº¦ä¸èƒ½é‡å¯†åº¦å…³ç³»ï¼šÎ©_gw(f) âˆ Î¾ * <A^2>ï¼ˆè§åæ–‡scaling_factoræ¨å¯¼ï¼‰
    
    ã€æ›¿ä»£æ–¹æ¡ˆã€‘
    - ABCï¼ˆApproximate Bayesian Computationï¼‰ï¼šæ‹’ç»é‡‡æ ·æ•ˆç‡ä½ï¼Œéœ€è¦O(10^6)æ¬¡æ¨¡æ‹Ÿ
    - BOLFIï¼ˆBayesian Optimization for Likelihood-Free Inferenceï¼‰ï¼šé€‚åˆä½ç»´å‚æ•°ï¼Œä½†Î¾å’ŒÎ©è”åˆæ¨æ–­æ—¶é‡‡æ ·æ•ˆç‡ä¸‹é™
    - ä¼ ç»Ÿç›´æ–¹å›¾/æ ¸å¯†åº¦ä¼°è®¡ï¼šç»´åº¦ç¾éš¾ï¼Œ4ç»´ç»Ÿè®¡é‡ç©ºé—´éœ€è¦æŒ‡æ•°çº§æ ·æœ¬
    
    ã€MAFä¼˜åŠ¿ã€‘
    - å½’ä¸€åŒ–æµä¿è¯æ¦‚ç‡å½’ä¸€åŒ–ï¼Œå¯ç›´æ¥è®¡ç®—è¯æ®ï¼ˆmodel comparisonï¼‰
    - å¯é€†å˜æ¢æ”¯æŒåéªŒé‡‡æ ·å’Œæ¦‚ç‡å¯†åº¦è¯„ä¼°
    - å¯¹å°–é”åéªŒï¼ˆå¦‚Î´å‡½æ•°å‹ä¼¼ç„¶ï¼‰æ¯”å˜åˆ†æ¨æ–­ï¼ˆVIï¼‰æ›´ç¨³å®š
    """
    
    def __init__(self, gpu_segments, scaling_factor, batch_size=256):
        self.segments = gpu_segments  # é¢„åŠ è½½çš„GPUæ•°æ®æ®µåˆ—è¡¨
        self.scaling_factor = scaling_factor  # ç‰©ç†æ ‡åº¦å› å­ï¼Œè§åç»­æ¨å¯¼
        self.batch_size = batch_size  # GPUå†…å­˜ä¼˜åŒ–çš„å…³é”®å‚æ•°
        self.seg_len = 8192  # 4ç§’ Ã— 2048Hz
        
        # é¢„è®¡ç®—æœ€å¤§åˆæ³•èµ·å§‹ç´¢å¼•ï¼Œé¿å…åˆ‡ç‰‡è¶Šç•Œï¼Œæå‡å¾ªç¯æ•ˆç‡
        self.max_idxs = [len(h1) - self.seg_len - 1 for h1, l1 in self.segments]
        
        print(f"   ğŸ”§ GPUæ¨¡æ‹Ÿå™¨å°±ç»ª: {len(self.segments)} ä¸ªæ•°æ®æº, "
              f"batch_size={batch_size}")
    
    def __call__(self, theta_batch):
        """
        ã€å‘é‡åŒ–æ¨¡æ‹Ÿã€‘
        ä¼ ç»Ÿå¾ªç¯ï¼šfor Î¸ in theta_batch: simulate(Î¸)  # Pythonå¼€é”€å¤§
        æœ¬å®ç°ï¼šæ‰¹é‡GPUå†…æ ¸è°ƒç”¨ï¼Œåˆ©ç”¨Tensor Coreå¹¶è¡Œ
        
        è¿”å›ç»Ÿè®¡ç‰¹å¾ xï¼Œè€ŒéåŸå§‹æ—¶é—´åºåˆ—ï¼Œå®ç°ç»´åº¦çº¦ç®€ï¼ˆ8192â†’4ï¼‰
        è¿™æ˜¯SBIçš„å…³é”®ï¼šå¯»æ‰¾å¯¹å‚æ•°Î¸å……åˆ†ç»Ÿè®¡é‡ï¼ˆsufficient statisticsï¼‰
        """
        if isinstance(theta_batch, np.ndarray):
            theta_batch = torch.tensor(theta_batch, dtype=torch.float32, device=device)
        
        n_samples = theta_batch.shape[0]
        results = []
        
        # åˆ†å—å¤„ç†ï¼šé¿å…GPUæ˜¾å­˜æº¢å‡ºï¼ˆOOMï¼‰
        for i in range(0, n_samples, self.batch_size):
            batch_end = min(i + self.batch_size, n_samples)
            theta_chunk = theta_batch[i:batch_end]
            batch_results = self._process_batch(theta_chunk)
            results.append(batch_results)
        
        return torch.cat(results, dim=0)
    
    def _process_batch(self, theta_batch):
        """
        ã€ä¿¡å·æ³¨å…¥ç®—æ³•ã€‘
        
        ç‰©ç†æ¨å¯¼ï¼šçˆ†å‘ä¿¡å·èƒ½é‡ä¸Omegaçš„å…³ç³»
        -----------------------------------
        å¯¹äºå ç©ºæ¯”ä¸ºÎ¾çš„burstä¿¡å·ï¼ŒGWèƒ½é‡å¯†åº¦å®šä¹‰ä¸ºï¼š
        Î©_gw(f) = (1/Ï_c) * (dÏ_gw/dln f)
        
        å…¶ä¸­èƒ½é‡å¯†åº¦ Ï_gw = (c^2/G) * <h^2> * (é¢‘ç‡å› å­)
        å¯¹äºç¦»æ•£burstï¼šh_rms^2 = Î¾ * A^2ï¼ˆAä¸ºå•æ¬¡çˆ†å‘åº”å˜ï¼‰
        
        é€šè¿‡é‡çº²åˆ†æå’ŒLIGOæ ¡å‡†ï¼Œå¾—åˆ°æ ‡åº¦å…³ç³»ï¼š
        A = sqrt(Î© / Î¾) * scaling_factor
        
        ã€å‚æ•°é€‰æ‹©ã€‘
        - scaling_factor: O3a=1300, O3b/O4a=1200
          åæ˜ ä¸åŒè§‚æµ‹å‘¨æœŸçš„æ¢æµ‹å™¨çµæ•åº¦å·®å¼‚ï¼ˆç”±è‡‚é•¿ã€æ¿€å…‰åŠŸç‡ã€å™ªå£°å†³å®šï¼‰
          æ•°å€¼é€šè¿‡æ³¨å…¥å·²çŸ¥ä¿¡å·çš„ä¿¡å™ªæ¯”åæ¼”æ ¡å‡†å¾—åˆ°
        
        ã€æ–¹æ³•å¯¹æ¯”ã€‘
        - é¢‘ç‡åŸŸæ³¨å…¥ï¼ˆFFTâ†’ä¹˜å“åº”å‡½æ•°â†’IFFTï¼‰ï¼šé€‚åˆè¿ç»­èƒŒæ™¯ï¼Œä½†burstéœ€è¦ç²¾ç¡®æ—¶é—´å®šä½
        - æ—¶é—´åŸŸç›´æ¥å åŠ ï¼ˆæœ¬æ–¹æ³•ï¼‰ï¼šä¿æŒburstçš„æ—¶é—´ç»“æ„ï¼Œç¬¦åˆç‰©ç†ç”Ÿæˆè¿‡ç¨‹
        
        ã€æ•°å€¼ä¼˜åŒ–ã€‘
        n_ev = (seg_len * safe_xi * 0.2)ï¼šæœŸæœ›çˆ†å‘æ¬¡æ•°
        0.2ä¸ºç»éªŒå› å­ï¼Œç¡®ä¿éç©ºä¿¡å·ä½†é¿å…è¿‡åº¦é‡å ï¼ˆburstç¢°æ’æ¦‚ç‡ä½ï¼‰
        """
        batch_size = theta_batch.shape[0]
        
        # éšæœºé€‰æ‹©æ•°æ®æºï¼šæ¨¡æ‹ŸçœŸå®åˆ†æä¸­ä»é•¿æ•°æ®æµéšæœºæŠ½å–æ®µ
        seg_indices = torch.randint(0, len(self.segments), (batch_size,), device=device)
        
        # æå–æ•°æ®æ®µï¼šæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹éšæœºèµ·å§‹ç‚¹ï¼Œå¢å¼ºè®­ç»ƒå¤šæ ·æ€§
        batch_data = []
        for idx in seg_indices:
            h1_full, l1_full = self.segments[idx.item()]
            max_idx = self.max_idxs[idx.item()]
            start = torch.randint(0, max_idx, (1,), device=device).item()
            
            h1_seg = h1_full[start:start+self.seg_len]
            l1_seg = l1_full[start:start+self.seg_len]
            batch_data.append((h1_seg, l1_seg))
        
        # å †å ä¸ºæ‰¹æ¬¡å¼ é‡ [B, seg_len]ï¼Œæ”¯æŒGPUå¹¶è¡Œè®¡ç®—
        h1_batch = torch.stack([d[0] for d in batch_data])
        l1_batch = torch.stack([d[1] for d in batch_data])
        
        # å‚æ•°è§£åŒ…ï¼šÎ¸ = [log10(Î©), Î¾]
        log_omega = theta_batch[:, 0]
        xi = theta_batch[:, 1]
        
        # ç‰©ç†é‡è®¡ç®—ï¼šç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        omega = 10.0 ** log_omega  # å¯¹æ•°åæ ‡è¦†ç›–å¤§åŠ¨æ€èŒƒå›´ï¼ˆ1e-13 è‡³ 0.1ï¼‰
        safe_xi = torch.clamp(xi, min=1e-4)  # é˜²æ­¢Î¾â†’0æ—¶çš„å¥‡å¼‚æ€§
        amp = torch.sqrt(omega / safe_xi) * self.scaling_factor  # çˆ†å‘å¹…åº¦
        
        # æ³Šæ¾è¿‡ç¨‹æ¨¡æ‹Ÿçˆ†å‘æ¬¡æ•°
        n_ev = (self.seg_len * safe_xi * 0.2).long()
        
        # ä¿¡å·ç”Ÿæˆï¼šç¨€ç–è„‰å†²åºåˆ—
        signals = torch.zeros(batch_size, self.seg_len, device=device)
        for b in range(batch_size):
            if n_ev[b] > 0:
                # éšæœºæ—¶é—´ä½ç½®ï¼šä½“ç°burstçš„éšæœºåˆ°è¾¾ç‰¹æ€§
                idx = torch.randint(0, self.seg_len, (n_ev[b].item(),), device=device)
                # é«˜æ–¯å¹…åº¦ï¼šå‡è®¾çˆ†å‘å¼ºåº¦æœä»å¯¹æ•°æ­£æ€ï¼ˆç»ä¸­å¿ƒæé™å®šç†å¹³æ»‘ï¼‰
                signals[b, idx] = torch.randn(n_ev[b].item(), device=device) * amp[b]
        
        # ä¿¡å·æ³¨å…¥ï¼šåŒæ¢æµ‹å™¨æ¥æ”¶ç›¸åŒä¿¡å·ï¼ˆå¿½ç•¥å¤©çº¿æ¨¡å¼å·®å¼‚ï¼Œç®€åŒ–æ¨¡å‹ï¼‰
        d_h1 = h1_batch + signals
        d_l1 = l1_batch + signals
        
        # è®¡ç®—å……åˆ†ç»Ÿè®¡é‡ï¼šé™ç»´è‡³4ç»´ç‰¹å¾ç©ºé—´
        features = self._compute_features(d_h1, d_l1)
        
        return features
    
    def _compute_features(self, d_h1, d_l1):
        """
        ã€å……åˆ†ç»Ÿè®¡é‡è®¾è®¡ã€‘
        
        SBIçš„æœ‰æ•ˆæ€§ä¾èµ–äºç»Ÿè®¡é‡ x å¯¹å‚æ•° Î¸ çš„å……åˆ†æ€§ã€‚
        æœ¬ä»£ç é€‰æ‹©4ä¸ªç‰©ç†é©±åŠ¨çš„ç»Ÿè®¡é‡ï¼Œä¸“é—¨é’ˆå¯¹éé«˜æ–¯æ€§æ£€æµ‹ï¼š
        
        1. çš®å°”é€Šç›¸å…³ç³»æ•° CCï¼š
           ã€ç‰©ç†æ„ä¹‰ã€‘æµ‹é‡H1-L1çš„å…³è”æ€§ã€‚SGWBåº”äº§ç”Ÿé«˜åº¦ç›¸å…³ä¿¡å·ï¼ˆCCâ‰ˆ1ï¼‰ï¼Œ
           è€Œæœ¬åœ°å™ªå£°ä¸ç›¸å…³ï¼ˆCCâ‰ˆ0ï¼‰ã€‚è¿™æ˜¯SGWBæ¢æµ‹çš„ç»å…¸ç»Ÿè®¡é‡ã€‚
           ã€è®¡ç®—æ–¹æ³•ã€‘r = Cov(H1,L1) / (Ïƒ_H1 * Ïƒ_L1)
        
        2. H1/L1å³°åº¦ï¼ˆKurtosisï¼‰ï¼š
           ã€ç‰©ç†æ„ä¹‰ã€‘æ ‡å‡†é«˜æ–¯å™ªå£°çš„å³°åº¦â‰ˆ3ï¼ˆè¶…é¢å³°åº¦â‰ˆ0ï¼‰ã€‚
           Burstä¿¡å·äº§ç”Ÿé‡å°¾åˆ†å¸ƒï¼ˆå³°åº¦>>3ï¼‰ï¼Œæ˜¯æ£€æµ‹éé«˜æ–¯æ€§çš„æ•æ„ŸæŒ‡æ ‡ã€‚
           ã€å¯¹æ•°å˜æ¢ã€‘log1p(|Îº|)å‹ç¼©åŠ¨æ€èŒƒå›´ï¼Œé€‚åº”MAFçš„è¾“å‡ºç¨³å®šæ€§ã€‚
        
        3. å¯¹æ•°åŠŸç‡ç§¯ PW = log10(Var(H1) * Var(L2))ï¼š
           ã€ç‰©ç†æ„ä¹‰ã€‘SGWBæ³¨å…¥ä¼šå¢åŠ æ¢æµ‹å™¨åŠŸç‡ï¼Œä¹˜ç§¯å½¢å¼æŠ‘åˆ¶å•æ¢æµ‹å™¨å™ªå£°æ¶¨è½ã€‚
        
        ã€å……åˆ†æ€§è®ºè¯ã€‘
        å¯¹äºé«˜æ–¯èƒŒæ™¯ï¼ŒCCå’ŒåŠŸç‡å®Œå…¨è¡¨å¾äºŒé˜¶ç»Ÿè®¡é‡ã€‚
        å¯¹äºéé«˜æ–¯burstï¼Œå³°åº¦æ•æ‰å››é˜¶çŸ©ä¿¡æ¯ã€‚
        ç»„åˆ{CC, K_H1, K_L1, PW}æ„æˆå¯¹(Î©, Î¾)çš„è¿‘ä¼¼å……åˆ†ç»Ÿè®¡é‡ã€‚
        
        ã€æ›¿ä»£ç‰¹å¾ã€‘
        - åŒè°±ï¼ˆBispectrumï¼‰ï¼šå¯¹éé«˜æ–¯æ€§æ›´æ•æ„Ÿï¼Œä½†è®¡ç®—å¤æ‚åº¦é«˜ï¼ˆO(N^2)ï¼‰
        - å°æ³¢ç³»æ•°ï¼šé€‚åˆéå¹³ç¨³ä¿¡å·ï¼Œä½†ç»´åº¦é«˜éœ€é¢å¤–é™ç»´ï¼ˆå¦‚è‡ªç¼–ç å™¨ï¼‰
        - å³°å€¼è®¡æ•°/è¶…è¶Šç‡ï¼šå¯¹burstæ•æ„Ÿä½†æŸå¤±å¹…åº¦ä¿¡æ¯
        
        ã€å½“å‰é€‰æ‹©ä¼˜åŠ¿ã€‘å¹³è¡¡è®¡ç®—æ•ˆç‡ï¼ˆO(N)ï¼‰ä¸ç»Ÿè®¡å……åˆ†æ€§ï¼Œ
        ä¸”å¯è§£é‡Šæ€§å¼ºï¼Œä¾¿äºç‰©ç†ç»“æœéªŒè¯ã€‚
        """
        batch_size = d_h1.shape[0]
        
        # 1. çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆGPUå‘é‡åŒ–å®ç°ï¼‰
        mean_h1 = d_h1.mean(dim=1, keepdim=True)
        mean_l1 = d_l1.mean(dim=1, keepdim=True)
        
        cent_h1 = d_h1 - mean_h1  # ä¸­å¿ƒåŒ–
        cent_l1 = d_l1 - mean_l1
        
        num = (cent_h1 * cent_l1).sum(dim=1)  # åæ–¹å·®
        den = torch.sqrt((cent_h1**2).sum(dim=1) * (cent_l1**2).sum(dim=1) + 1e-10)  # æ ‡å‡†å·®ç§¯
        cc = num / den  # [-1, 1]èŒƒå›´
        
        # 2. å³°åº¦è®¡ç®—ï¼ˆç®€åŒ–å…¬å¼é¿å…scipyçš„CPUå¾€è¿”ï¼‰
        # æ ‡å‡†å³°åº¦ï¼šÎº = E[(X-Î¼)^4] / Ïƒ^4
        std_h1 = torch.sqrt((cent_h1**2).mean(dim=1))
        std_l1 = torch.sqrt((cent_l1**2).mean(dim=1))
        
        z_h1 = cent_h1 / (std_h1.unsqueeze(1) + 1e-10)  # æ ‡å‡†åŒ–
        z_l1 = cent_l1 / (std_l1.unsqueeze(1) + 1e-10)
        
        k_h1_raw = (z_h1**4).mean(dim=1)  # å››é˜¶çŸ©
        k_l1_raw = (z_l1**4).mean(dim=1)
        
        # å¯¹æ•°å˜æ¢ï¼šå¤„ç†é‡å°¾åˆ†å¸ƒçš„å·¨å¤§åŠ¨æ€èŒƒå›´
        k_h1 = torch.log1p(torch.abs(k_h1_raw))
        k_l1 = torch.log1p(torch.abs(k_l1_raw))
        
        # 3. å¯¹æ•°åŠŸç‡ç§¯ï¼šä¹˜ç§¯å½¢å¼å¢å¼ºåŒæ¢æµ‹å™¨ä¸€è‡´æ€§çº¦æŸ
        var_h1 = (d_h1**2).mean(dim=1)  # å‡è®¾å·²ä¸­å¿ƒåŒ–ï¼ˆå‡å€¼ä¸º0ï¼‰
        var_l1 = (d_l1**2).mean(dim=1)
        pw = torch.log10(var_h1 * var_l1 + 1e-30)  # é˜²æ­¢log(0)
        
        # ç‰¹å¾å †å ï¼š[B, 4]
        features = torch.stack([cc, k_h1, k_l1, pw], dim=1)
        
        return features.float()

# ==========================================
# 5. å‘ç°æ•°æ®
# ==========================================
print(f"{'='*60}")
print("ğŸ” Phase 9: GPUåŠ é€Ÿç‰ˆå¤šæ•°æ®ç»„åˆ†æ")
print(f"{'='*60}")

print(f"\nğŸ“‚ æ‰«ææ•°æ®ç›®å½•: {DATA_DIR}")
data_groups = discover_data_groups(DATA_DIR)

AVAILABLE_DATASETS = sorted([k for k in data_groups.keys() if len(data_groups[k]) > 0])
if len(AVAILABLE_DATASETS) == 0:
    raise RuntimeError("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®")

print(f"\nğŸ¯ å°†å¤„ç†: {AVAILABLE_DATASETS}")

# ==========================================
# 6. ä¸»è®­ç»ƒå¾ªç¯ (æŒ‰ç»„å•ç‹¬è®­ç»ƒ)
# ==========================================
"""
ã€å®éªŒè®¾è®¡ã€‘åˆ†æ•°æ®ç»„ç‹¬ç«‹è®­ç»ƒç­–ç•¥

ã€åŠ¨æœºã€‘
ä¸åŒæ—¶é—´æ®µçš„å™ªå£°éå¹³ç¨³æ€§ï¼ˆnon-stationarityï¼‰å¯¼è‡´æ¡ä»¶åˆ†å¸ƒ p(x|Î¸) éšæ—¶é—´å˜åŒ–ã€‚
è‹¥æ··åˆæ‰€æœ‰æ•°æ®è®­ç»ƒï¼Œç¥ç»ç½‘ç»œè¢«è¿«å­¦ä¹ å¹³å‡åˆ†å¸ƒï¼ŒæŸå¤±å¯¹ç‰¹å®šå™ªå£°å®ç°çš„æœ€ä¼˜æ€§ã€‚

ã€ç‰©ç†ä¾æ®ã€‘
LIGOå™ªå£°åŒ…å«ï¼š
- çª„çº¿è°±ï¼ˆ60Hzç”µåŠ›è°æ³¢ã€ violin modes ç­‰ï¼‰
- éå¹³ç¨³ glitch èƒŒæ™¯
- ä½é¢‘éå¹³ç¨³ï¼ˆåœ°éœ‡å™ªå£°ï¼‰

è¿™äº›ç‰¹å¾åœ¨ä¸åŒè§‚æµ‹å‘¨æœŸï¼ˆO3a/O3b/O4aï¼‰å’Œä¸åŒæ—¶é—´æ®µå·®å¼‚æ˜¾è‘—ã€‚

ã€æ–¹æ³•å¯¹æ¯”ã€‘
- å…¨å±€è®­ç»ƒ+æ¡ä»¶ç¥ç»ç½‘ç»œï¼ˆcSBIï¼‰ï¼šç†è®ºä¸Šå¯å­¦ä¹ æ—¶é—´ä¾èµ–ï¼Œä½†éœ€è¦æ›´å¤šæ•°æ®
- è¿ç§»å­¦ä¹ ï¼šå…ˆå…¨å±€é¢„è®­ç»ƒï¼Œå†ç‰¹å®šæ•°æ®ç»„å¾®è°ƒï¼Œé€‚åˆæ•°æ®ç¨€ç¼ºåœºæ™¯
- ç‹¬ç«‹è®­ç»ƒï¼ˆæœ¬æ–¹æ³•ï¼‰ï¼šæœ€å¤§åŒ–å„æ•°æ®ç»„æ€§èƒ½ï¼Œå‡è®¾ç»„é—´æ— å…±äº«ç»“æ„

ã€é€‰æ‹©ç†ç”±ã€‘
SGWBæœç´¢å¤„äºçµæ•åº¦æé™ï¼Œä»»ä½•æ€§èƒ½æŸå¤±éƒ½å½±å“ç§‘å­¦å‘ç°ã€‚
ç‹¬ç«‹è®­ç»ƒç¡®ä¿æ¯ä¸ªæ•°æ®ç»„çš„æœ€ä¼˜åéªŒè¿‘ä¼¼ã€‚
"""
N_SIMS = 10000  # ã€æ¨¡æ‹Ÿæ€»æ•°ã€‘å®šä¹‰è®­ç»ƒé›†çš„è§„æ¨¡ã€‚10kæ ·æœ¬æ˜¯SBIçš„å…¸å‹åŸºå‡†ï¼Œè¶³ä»¥è®­ç»ƒMAFç½‘ç»œã€‚
BATCH_SIZE = 512  # ã€æ˜¾å­˜æ§åˆ¶ã€‘æ‰¹å¤§å°ã€‚é™åˆ¶æ¯æ¬¡é€å…¥GPUçš„æ•°æ®é‡ï¼Œé˜²æ­¢8192ç‚¹çš„æ—¶é—´åºåˆ—æ’‘çˆ†æ˜¾å­˜ã€‚
CURVE_DATA = {}   # ã€ç»“æœå®¹å™¨ã€‘ç”¨äºå­˜å‚¨æœ€ç»ˆè®¡ç®—å‡ºçš„çµæ•åº¦æ›²çº¿æ•°æ®ï¼Œç”¨äºåç»­ç”»å›¾ã€‚

# ã€å…ˆéªŒåˆ†å¸ƒå®šä¹‰ã€‘
# è¿™æ˜¯è´å¶æ–¯æ¨æ–­çš„èµ·ç‚¹ p(Î¸)ã€‚
# BoxUniform è¡¨ç¤ºåœ¨çŸ©å½¢åŒºåŸŸå†…çš„å‡åŒ€åˆ†å¸ƒã€‚
# å‚æ•° Î¸ = [log10(Î©), Î¾]
# ç»´åº¦ 0 (log_Î©): [-13, -1]ã€‚ç‰©ç†æ„ä¹‰ï¼šä»å®Œå…¨ä¸å¯æ¢æµ‹çš„å¾®å¼±ä¿¡å·(-13)åˆ°æå¼ºçš„èƒŒæ™¯(-1)ã€‚
# ç»´åº¦ 1 (Î¾): [0.001, 1.0]ã€‚ç‰©ç†æ„ä¹‰ï¼šä»æç¨€ç–çš„"çˆ†ç±³èŠ±"ä¿¡å·(0.1%)åˆ°è¿ç»­çš„é«˜æ–¯èƒŒæ™¯(100%)ã€‚
prior = BoxUniform(
    low=torch.tensor([-13.0, 0.001], device=device), 
    high=torch.tensor([-1.0, 1.0], device=device)
)

# ==========================================
# å¾ªç¯éå†æ‰€æœ‰é€‰å®šçš„æ•°æ®é›† (å¦‚ 'O3a', 'O3b', 'O4a')
# ==========================================
for dataset in AVAILABLE_DATASETS:
    print(f"\n{'='*60}")
    print(f"ğŸ”¹ å¤„ç†æ•°æ®é›†: {dataset}")
    print(f"{'='*60}")
    
    # -------------------------------------------------------
    # 1. ã€ç‰©ç†æ ‡åº¦æ ¡å‡† (Scaling Factor Calibration)ã€‘
    # -------------------------------------------------------
    # åŠŸèƒ½ï¼šæ ¹æ®å½“å‰æ¢æµ‹å™¨çš„çµæ•åº¦ï¼Œè®¾ç½®æ³¨å…¥ä¿¡å·çš„å½’ä¸€åŒ–å› å­ã€‚
    # ç‰©ç†èƒŒæ™¯ï¼š
    #   æ¢æµ‹å™¨çµæ•åº¦è¶Šå·®(å™ªå£°è¶Šé«˜)ï¼Œä¸ºäº†è¾¾åˆ°åŒæ ·çš„ä¿¡å™ªæ¯”(SNR)ï¼Œ
    #   æˆ‘ä»¬éœ€è¦æ³¨å…¥æ›´å¼ºçš„ä¿¡å·æŒ¯å¹…ã€‚
    #   å…¬å¼: Amplitude = sqrt(Î© / Î¾) * scaling_factor
    # -------------------------------------------------------
    if dataset == "O3a":
        # O3a æ—¶æœŸçµæ•åº¦è¾ƒ O3b/O4 ç•¥ä½ï¼Œå™ªå£°åŸºåº•è¾ƒé«˜ã€‚
        # å› æ­¤ä½¿ç”¨è¾ƒå¤§çš„å› å­ (1300) æ¥ç¡®ä¿æ³¨å…¥çš„ä¿¡å·èƒ½è¢«â€œçœ‹è§â€ã€‚
        scaling_factor = 1300.0
        
    elif dataset in ["O3b", "O4a"]:
        # O3b å’Œ O4a çµæ•åº¦æå‡ï¼Œå™ªå£°åŸºåº•é™ä½ã€‚
        # ä½¿ç”¨è¾ƒå°çš„å› å­ (1200) å³å¯è¾¾åˆ°åŒç­‰ SNRï¼Œé¿å…ä¿¡å·æº¢å‡ºæˆ–è¿‡äºå¤¸å¼ ã€‚
        scaling_factor = 1200.0
        
    else:
        # å¯¹äºæœªçŸ¥æ•°æ®é›†ï¼Œè®¾ç½®ä¸€ä¸ªéå¸¸ä¿å®ˆçš„é«˜å› å­ï¼Œ
        # ç¡®ä¿ä¿¡å·è¶³å¤Ÿå¼ºï¼Œé˜²æ­¢å› ä¸ºæ ¡å‡†ä¸å½“å¯¼è‡´è®­ç»ƒå‡ºçš„ç½‘ç»œä»€ä¹ˆéƒ½å­¦ä¸åˆ°ã€‚
        scaling_factor = 3000.0  # ä¿å®ˆé»˜è®¤å€¼
    
    # è·å–è¯¥æ•°æ®é›†ä¸‹æ‰€æœ‰çš„æ•°æ®åˆ†æ®µï¼ˆGroupsï¼‰åˆ—è¡¨
    groups_list = data_groups[dataset]
    print(f"   ğŸ“Š å…± {len(groups_list)} ä¸ªæ•°æ®ç»„")
    
    # ==========================================
    # å†…éƒ¨å¾ªç¯ï¼šéå†è¯¥æ•°æ®é›†ä¸‹çš„æ¯ä¸€ä¸ªæ•°æ®ç»„
    # ==========================================
    for group_idx, group in enumerate(groups_list):
        print(f"\n{'='*80}")
        print(f"ğŸ”¸ å¤„ç†æ•°æ®ç»„ {group_idx+1}/{len(groups_list)}: {group['name']}")
        print(f"{'='*80}")
        
        # -------------------------------------------------------
        # 2. ã€æ•°æ®åŠ è½½ä¸é¢„å¤„ç† (IO & Preprocessing)ã€‘
        # -------------------------------------------------------
        # å‡½æ•°ï¼šload_and_preprocess_to_gpu(group)
        # è¾“å…¥ï¼šgroup å­—å…¸ (åŒ…å« 'h1', 'l1' æ–‡ä»¶è·¯å¾„)
        # è¾“å‡ºï¼š(h1_gpu, l1_gpu) PyTorch Tensorï¼Œä¸”å·²åœ¨ GPU ä¸Š
        # -------------------------------------------------------
        print(f"   ğŸ“¥ åŠ è½½æ•°æ®ç»„: {group['name']}")
        h1_gpu, l1_gpu = load_and_preprocess_to_gpu(group)
        
        # é”™è¯¯å¤„ç†ï¼šå¦‚æœæ–‡ä»¶æŸåæˆ–è·¯å¾„é”™è¯¯ï¼Œå‡½æ•°è¿”å› Noneï¼Œæ­¤å¤„è·³è¿‡ä¸å´©æºƒ
        if h1_gpu is None:
            print(f"   âš ï¸ è·³è¿‡æ•°æ®ç»„: {group['name']} (åŠ è½½å¤±è´¥)")
            continue
        
        # -------------------------------------------------------
        # 3. ã€æ¨¡æ‹Ÿå™¨å®ä¾‹åŒ– (Simulator Instantiation)ã€‘
        # -------------------------------------------------------
        # ç±»ï¼šGPUBatchSimulator
        # ä½œç”¨ï¼šè¿™æ˜¯ç‰©ç†ä»¿çœŸå¼•æ“ã€‚å®ƒå°†æŒæœ‰çš„çœŸå®å™ªå£°æ•°æ®å’Œç‰©ç†å‚æ•°ç»‘å®šã€‚
        # å…³é”®å‚æ•°ï¼š
        #   - gpu_segments: [(h1, l1)] çœŸå®çš„èƒŒæ™¯å™ªå£°ç‰‡æ®µï¼ˆå·²åœ¨æ˜¾å­˜ä¸­ï¼‰ã€‚
        #   - scaling_factor: ä¸Šé¢æ ¹æ®æ•°æ®é›†ç¡®å®šçš„æ ¡å‡†å› å­ã€‚
        #   - batch_size: å†³å®šäº†ä¸€æ¬¡å¹¶è¡Œç”Ÿæˆå¤šå°‘ä¸ªæ ·æœ¬ã€‚
        # -------------------------------------------------------
        
        # å°†åŠ è½½å¥½çš„ GPU å¼ é‡æ‰“åŒ…æˆåˆ—è¡¨ (æ”¯æŒå¤šæ®µæ•°æ®ï¼Œè¿™é‡Œæš‚æ—¶åªæ”¾ä¸€æ®µ)
        gpu_segments = [(h1_gpu, l1_gpu)]
        
        # å®ä¾‹åŒ–æ¨¡æ‹Ÿå™¨å¯¹è±¡
        # æ³¨æ„ï¼šæ­¤æ—¶å¹¶æ²¡æœ‰ç”Ÿæˆä»»ä½•æ¨¡æ‹Ÿæ•°æ®ï¼Œåªæ˜¯å‡†å¤‡å¥½äº†ç¯å¢ƒï¼ˆåŠ è½½äº†å™ªå£°èƒŒæ™¯ï¼‰ã€‚
        # çœŸæ­£çš„æ•°æ®ç”Ÿæˆä¼šåœ¨åç»­è°ƒç”¨ simulator(theta) æ—¶è§¦å‘ã€‚
        simulator = GPUBatchSimulator(gpu_segments, scaling_factor, batch_size=BATCH_SIZE)
        
        # (åœ¨æ­¤å¤„ä¹‹åï¼Œé€šå¸¸ç´§æ¥ç€å°±æ˜¯æ¨¡æ‹Ÿå¾ªç¯ for i in range(N_SIMS)...)
        
        # å®ä¾‹åŒ–æ¨¡æ‹Ÿå™¨ï¼šç»‘å®šå½“å‰æ•°æ®ç»„çš„å™ªå£°å®ç°
        gpu_segments = [(h1_gpu, l1_gpu)]
        simulator = GPUBatchSimulator(gpu_segments, scaling_factor, batch_size=BATCH_SIZE)
        
        # ==========================================
        # æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
        # ==========================================
        print(f"   ğŸ”„ ç”Ÿæˆ {N_SIMS} ä¸ªæ¨¡æ‹Ÿæ ·æœ¬ (GPUåŠ é€Ÿ)...")
        theta_tr_list = []
        x_tr_list = []
        
        n_batches = (N_SIMS + BATCH_SIZE - 1) // BATCH_SIZE
        
        for i in tqdm(range(n_batches), desc="GPUæ¨¡æ‹Ÿ"):
            current_batch = min(BATCH_SIZE, N_SIMS - i * BATCH_SIZE)
            
            # ä»å…ˆéªŒé‡‡æ ·
            theta_batch = prior.sample((current_batch,))
            
            # GPUæ¨¡æ‹Ÿï¼šæ ¸å¿ƒè®¡ç®—ç“¶é¢ˆ
            x_batch = simulator(theta_batch)
            
            # ç§»è‡³CPUé‡Šæ”¾æ˜¾å­˜ï¼ˆç¥ç»ç½‘ç»œè®­ç»ƒåœ¨GPUï¼Œä½†æ•°æ®å­˜å‚¨åœ¨CPUï¼‰
            theta_tr_list.append(theta_batch.cpu())
            x_tr_list.append(x_batch.cpu())
            
            # æ˜¾å­˜ç®¡ç†ï¼šå®šæœŸæ¸…ç†CUDAç¼“å­˜ï¼Œé˜²æ­¢ç¢ç‰‡åŒ–
            if i % 10 == 0:
                torch.cuda.empty_cache()
        
        theta_tr = torch.cat(theta_tr_list, dim=0)  # [N_SIMS, 2]
        x_tr = torch.cat(x_tr_list, dim=0)          # [N_SIMS, 4]
        
        print(f"   âœ… æ¨¡æ‹Ÿå®Œæˆ: theta={theta_tr.shape}, x={x_tr.shape}")
        
        # ==========================================
        # ING-Net (AI) è®­ç»ƒï¼šä½¿ç”¨å…¨éƒ¨4ä¸ªç‰¹å¾
        # ==========================================
        print("   ğŸ¤– è®­ç»ƒ ING-Net (AI)...")
        """
        ã€ç¥ç»ç½‘ç»œæ¶æ„ã€‘MAFï¼ˆMasked Autoregressive Flowï¼‰
        
        MAFé€šè¿‡è‡ªå›å½’åˆ†è§£å­¦ä¹ é«˜ç»´åˆ†å¸ƒï¼š
        p(Î¸|x) = Î  p(Î¸_i | Î¸_{<i}, x)
        
        æ¯æ­¥ä½¿ç”¨æ©ç è‡ªå›å½’ç½‘ç»œä¿è¯å› æœæ€§ï¼Œ
        åŸºåˆ†å¸ƒä¸ºæ ‡å‡†é«˜æ–¯ï¼Œé€šè¿‡å¯é€†å˜æ¢å¾—åˆ°å¤æ‚åéªŒã€‚
        
        ã€è®­ç»ƒé…ç½®ã€‘
        - training_batch_size=10000ï¼šå¤§æ‰¹é‡ç¨³å®šæ¢¯åº¦ä¼°è®¡
        - use_combined_loss=Trueï¼šåŒæ—¶ä¼˜åŒ–è®­ç»ƒé›†ä¼¼ç„¶å’Œæè®®åˆ†å¸ƒï¼ˆSNPE-Cç¨³å®šæ€§ï¼‰
        """

# ==============================================================================
# 1. å®ä¾‹åŒ–æ¨ç†å™¨ (Inference Object Initialization)
# ==============================================================================
# SNPE (Sequential Neural Posterior Estimation): 
# ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„ç®—æ³•ï¼Œç›®æ ‡æ˜¯ç›´æ¥å­¦ä¹ åéªŒæ¦‚ç‡ p(Î¸|x)ã€‚
#
# å‚æ•°è¯¦è§£:
# - prior: ç‰©ç†å‚æ•°çš„å…ˆéªŒèŒƒå›´ (ä¹‹å‰å®šä¹‰çš„ BoxUniform)ã€‚
#          å‘Šè¯‰ç¥ç»ç½‘ç»œå‚æ•° Î¸ (Î©, Î¾) çš„ç‰©ç†è¾¹ç•Œï¼Œé˜²æ­¢é¢„æµ‹å‡ºä¸åˆé€»è¾‘çš„å€¼ã€‚
# - density_estimator="maf": æ ¸å¿ƒç¥ç»ç½‘ç»œæ¶æ„ã€‚
#          "maf" (Masked Autoregressive Flow) æ˜¯ä¸€ç§â€œæ­£åˆ™åŒ–æµâ€æ¨¡å‹ã€‚
#          å®ƒçš„ä¼˜åŠ¿åœ¨äºèƒ½ç²¾ç¡®æ¨¡æ‹Ÿæå…¶å¤æ‚çš„ã€éé«˜æ–¯çš„ã€å¤šå³°çš„æ¦‚ç‡åˆ†å¸ƒã€‚
#          (è¿™å¯¹äºæ•æ‰éé«˜æ–¯ Burst ä¿¡å·äº§ç”Ÿçš„å¤æ‚åéªŒè‡³å…³é‡è¦)ã€‚
# - device: æŒ‡å®šåœ¨ GPU ä¸Šè¿è¡Œï¼ŒåŠ é€Ÿè®­ç»ƒã€‚
        inf_ai = SNPE(prior=prior, density_estimator="maf", device=device)

# ==============================================================================
# 2. æ³¨å…¥è®­ç»ƒæ•°æ® (Loading Simulation Data)
# ==============================================================================
# å°†ä¹‹å‰ç‰©ç†æ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„â€œæˆå¯¹æ•°æ®â€å–‚ç»™æ¨ç†å™¨ã€‚
# - theta_tr: çœŸå€¼ (Ground Truth)ï¼Œå³ [logÎ©, Î¾]ã€‚å½¢çŠ¶ [N, 2]ã€‚
# - x_tr:     è§‚æµ‹ç‰¹å¾ (Summary Statistics)ï¼Œå³ [CC, K_H, K_L, PW]ã€‚å½¢çŠ¶ [N, 4]ã€‚
#
# è¿™ä¸€æ­¥ç›¸å½“äºç»™ç¥ç»ç½‘ç»œæä¾›äº†å‡ ä¸‡é“â€œç»ƒä¹ é¢˜â€å’Œå¯¹åº”çš„â€œæ ‡å‡†ç­”æ¡ˆâ€ã€‚
        inf_ai.append_simulations(theta_tr, x_tr)

# ==============================================================================
# 3. è®­ç»ƒç½‘ç»œå¹¶æ„å»ºåéªŒ (Training & Building Posterior)
# ==============================================================================
# è¿™é‡ŒåµŒå¥—äº†ä¸¤ä¸ªåŠ¨ä½œï¼šå…ˆè®­ç»ƒ(train)ï¼Œåæ„å»º(build)ã€‚
#
# inf_ai.train(...) çš„ä½œç”¨:
# æ‰§è¡Œç¥ç»ç½‘ç»œçš„æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚å®ƒè°ƒæ•´ MAF ç½‘ç»œçš„æƒé‡ï¼Œ
# ä½¿å¾—ç½‘ç»œåœ¨ç»™å®šç‰¹å¾ x æ—¶ï¼Œé¢„æµ‹å‡ºçš„ Î¸ åˆ†å¸ƒå°½å¯èƒ½æ¥è¿‘çœŸå®çš„ Î¸ã€‚
#
# è®­ç»ƒå‚æ•°è¯¦è§£:
# - training_batch_size=10000: 
#   ä½¿ç”¨å…¨é‡æ•°æ®(Full Batch)è¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚
#   å› ä¸ºæ¨¡æ‹Ÿæ•°æ®é‡(10k)å¯¹äºGPUæ˜¾å­˜æ¥è¯´å¾ˆå°ï¼Œå…¨é‡è®­ç»ƒèƒ½ä½¿æ¢¯åº¦ä¼°è®¡éå¸¸å¹³æ»‘ã€ç¨³å®šï¼Œ
#   é¿å…å°æ‰¹é‡(Mini-batch)å¸¦æ¥çš„éšæœºéœ‡è¡ã€‚
# - show_train_summary=False: ä¸æ‰“å°å†—é•¿çš„è®­ç»ƒè¿›åº¦æ¡ï¼Œä¿æŒæ—¥å¿—æ•´æ´ã€‚
# - use_combined_loss=True: 
#   sbi åº“çš„ä¸€ä¸ªé«˜çº§æŠ€å·§ã€‚å®ƒæ··åˆäº†æœ€å¤§ä¼¼ç„¶æŸå¤±(ML)å’Œå…¶ä»–è¾…åŠ©æŸå¤±ï¼Œ
#   èƒ½æ˜¾è‘—æé«˜ç½‘ç»œåœ¨è®­ç»ƒåˆæœŸçš„ç¨³å®šæ€§ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸æˆ–æ¨¡å¼åå¡Œã€‚
#
# inf_ai.build_posterior(...) çš„ä½œç”¨:
# è®­ç»ƒç»“æŸåï¼Œinf_ai åªæ˜¯ä¸€ä¸ªåŒ…å«æƒé‡çš„â€œç©ºå£³â€ã€‚
# build_posterior å°†è®­ç»ƒå¥½çš„ç½‘ç»œå°è£…æˆä¸€ä¸ªå¯è°ƒç”¨çš„æ¦‚ç‡åˆ†å¸ƒå¯¹è±¡ post_aiã€‚
# ä»¥åä½ æ‹¿åˆ°çœŸå®çš„å¼•åŠ›æ³¢æ•°æ® x_realï¼Œåªéœ€è°ƒç”¨ post_ai.sample(x=x_real) 
# å°±èƒ½å¾—åˆ°å‚æ•°çš„æµ‹é‡ç»“æœã€‚
        post_ai = inf_ai.build_posterior(
            inf_ai.train(
                training_batch_size=10000,
                show_train_summary=False,
                use_combined_loss=True
            )
        )

        # æ˜¾å­˜æ¸…ç†ï¼šåˆ é™¤å¤§å‹ä¸­é—´å¼ é‡
        del theta_tr_list, x_tr_list
        torch.cuda.empty_cache()
        
        # ==========================================
        # Traditional è®­ç»ƒï¼šä»…ä½¿ç”¨CCå’ŒPWï¼ˆäºŒé˜¶ç»Ÿè®¡é‡ï¼‰
        # ==========================================
        print("   ğŸ“Š è®­ç»ƒ Traditional...")
        """
        ã€å¯¹æ¯”åŸºçº¿æ–¹æ³•ã€‘
        
        Traditionalæ–¹æ³•ä»…ä½¿ç”¨ï¼š
        - CCï¼ˆäº’ç›¸å…³ï¼‰ï¼šæ ‡å‡†SGWBæ¢æµ‹ç»Ÿè®¡é‡ï¼ˆAllen & Romano 1999ï¼‰
        - PWï¼ˆåŠŸç‡ç§¯ï¼‰ï¼šå•æ¢æµ‹å™¨åŠŸç‡ç›‘æµ‹
        
        æ˜¾å¼æ’é™¤å³°åº¦ï¼ˆK_H1, K_L1ï¼‰ï¼Œæ„æˆ"é«˜æ–¯è¿‘ä¼¼"åŸºçº¿ã€‚
        è¿™æ¨¡æ‹Ÿäº†ä¼ ç»ŸéšæœºèƒŒæ™¯æœç´¢ï¼ˆå‡è®¾èƒŒæ™¯ä¸ºé«˜æ–¯ï¼‰çš„è¡¨ç°ã€‚
        
        ã€ç§‘å­¦é—®é¢˜ã€‘
        é€šè¿‡å¯¹æ¯”ING-Netï¼ˆ4ç‰¹å¾ï¼‰å’ŒTraditionalï¼ˆ2ç‰¹å¾ï¼‰ï¼Œ
        é‡åŒ–éé«˜æ–¯ä¿¡æ¯ï¼ˆå³°åº¦ï¼‰å¯¹burst SGWBæ¢æµ‹çš„æå‡ã€‚
        """
# ==============================================================================
# 1. åˆå§‹åŒ–â€œä¼ ç»Ÿæ–¹æ³•â€æ¨ç†å™¨ (Baseline Model Initialization)
# ==============================================================================
# ç›®çš„ï¼š0å»ºç«‹ä¸€ä¸ªå¯¹ç…§ç»„ã€‚è™½ç„¶å®ƒåº•å±‚ç”¨çš„è¿˜æ˜¯ç¥ç»ç½‘ç»œ(SNPE/MAF)ï¼Œ
# ä½†æˆ‘ä»¬ä¼šé™åˆ¶å®ƒçš„è¾“å…¥ï¼Œè®©å®ƒæ— æ³•çœ‹åˆ°éé«˜æ–¯ä¿¡æ¯ï¼Œä»è€Œæ¨¡æ‹Ÿä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚
        inf_tr = SNPE(prior=prior, density_estimator="maf", device=device)
# ==============================================================================
# 2. æ³¨å…¥â€œé˜‰å‰²ç‰ˆâ€æ•°æ® (Feature Selection / Masking)
# ==============================================================================
# ã€æ ¸å¿ƒå·®å¼‚ç‚¹ã€‘
# x_tr[:, [0, 3]] æ˜¯ Python çš„åˆ‡ç‰‡æ“ä½œï¼š
# - index 0: CC (Cross-Correlation, äº’ç›¸å…³) -> ä»£è¡¨ä¼ ç»Ÿçš„èƒ½é‡æ¢æµ‹ã€‚
# - index 3: PW (Power, åŠŸç‡) -> ä»£è¡¨å•æ¢æµ‹å™¨çš„èƒ½é‡ç›‘æµ‹ã€‚
# å…³é”®ç‚¹ï¼šæ˜¾å¼ä¸¢å¼ƒäº† index 1 å’Œ 2 (å³°åº¦ Kurtosis)ã€‚
# ç‰©ç†æ„ä¹‰ï¼šå¼ºè¡Œè®©ç½‘ç»œæ— æ³•æ„ŸçŸ¥ä¿¡å·çš„â€œå°–é”ç¨‹åº¦â€(éé«˜æ–¯æ€§)ã€‚
# è¿™ä½¿å¾— post_tr çš„é¢„æµ‹ç»“æœç­‰æ•ˆäºåŸºäºé«˜æ–¯å‡è®¾çš„ä¼ ç»Ÿç®—æ³•ã€‚
        inf_tr.append_simulations(theta_tr, x_tr[:, [0, 3]])  # ä»…ä¿ç•™äºŒé˜¶ç»Ÿè®¡é‡(CC, PW)
# ==============================================================================
# 3. è®­ç»ƒå¯¹ç…§ç»„æ¨¡å‹ (Training Baseline)
# ==============================================================================
# å¯åŠ¨è®­ç»ƒå¾ªç¯ã€‚
# build_posterior: å°†è®­ç»ƒç»“æœå°è£…æˆæ¦‚ç‡åˆ†å¸ƒå¯¹è±¡ã€‚
# train(...): æ‰§è¡Œæ¢¯åº¦ä¸‹é™ã€‚å‚æ•°ä¸ AI æ¨¡å‹ä¿æŒä¸€è‡´ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒã€‚
        post_tr = inf_tr.build_posterior(
            inf_tr.train(training_batch_size=10000, show_train_summary=False)
        )
# ==============================================================================
# 4. æ¨¡å‹æŒä¹…åŒ– (Model Saving)
# ==============================================================================
        print("   ğŸ’¾ ä¿å­˜æ¨¡å‹...")  # æ‰“å°æ—¥å¿—ï¼Œæç¤ºç”¨æˆ·æ­£åœ¨å†™ç›˜
# æ‹¼æ¥æ–‡ä»¶ä¿å­˜è·¯å¾„ã€‚
# os.path.join: è‡ªåŠ¨å¤„ç†ä¸åŒæ“ä½œç³»ç»Ÿçš„è·¯å¾„åˆ†éš”ç¬¦ (Windows '\', Linux '/')ã€‚
# æ–‡ä»¶ååŒ…å« group['name'] (å¦‚ 'Chunk1')ï¼Œä¿è¯æ¯ä¸ªæ•°æ®æ®µçš„æ¨¡å‹äº’ä¸è¦†ç›–ã€‚
        model_ai_path = os.path.join(MODEL_DIR, f"{group['name']}_ai_model.pt")
        model_tr_path = os.path.join(MODEL_DIR, f"{group['name']}_traditional_model.pt")
# ä½¿ç”¨ PyTorch çš„åºåˆ—åŒ–å·¥å…·ä¿å­˜å¯¹è±¡ã€‚
# æ³¨æ„ï¼šè¿™é‡Œä¿å­˜çš„æ˜¯ post_ai (ä¸Šä¸€æ®µä»£ç è®­ç»ƒçš„å…¨ç‰¹å¾æ¨¡å‹) å’Œ post_tr (åˆšè®­ç»ƒçš„)ã€‚
# ä¿å­˜æ•´ä¸ªå¯¹è±¡(Object)è€Œä¸ä»…ä»…æ˜¯æƒé‡(State Dict)ï¼Œè¿™æ ·åç»­åŠ è½½æ—¶å¯ä»¥ç›´æ¥ç”¨æ¥é‡‡æ ·ï¼Œ
# ä¸éœ€è¦é‡æ–°å®šä¹‰ç½‘ç»œç»“æ„ã€‚
        torch.save(post_ai, model_ai_path)  # ä¿å­˜ ING-Net (4ç‰¹å¾)
        torch.save(post_tr, model_tr_path)  # ä¿å­˜ Traditional (2ç‰¹å¾)
# æ‰“å°æˆåŠŸæ—¥å¿—ï¼Œç¡®è®¤æ–‡ä»¶å·²ç”Ÿæˆã€‚
        print(f"   âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_ai_path}")
        print(f"   âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_tr_path}")
        # ==========================================
        # çµæ•åº¦æé™è®¡ç®—
        # ==========================================
        print("   ğŸ“ˆ è®¡ç®—çµæ•åº¦æé™...")
        """
        ã€çµæ•åº¦å®šä¹‰ã€‘
        é‡‡ç”¨"5%åˆ†ä½æ•°é˜ˆå€¼"å®šä¹‰ï¼š
        å¯¹äºç»™å®šÎ¾ï¼Œæ‰¾åˆ°æœ€å°çš„Î©ä½¿å¾—åéªŒçš„5%åˆ†ä½æ•° > -11ã€‚
        ç‰©ç†æ„ä¹‰ï¼š95%ç½®ä¿¡æ°´å¹³ä¸‹å¯åŒºåˆ†äºé›¶ï¼ˆÎ©=0ï¼‰çš„æœ€å°èƒ½é‡å¯†åº¦ã€‚
        ã€ç®—æ³•ã€‘çº¿æ€§æ‰«æ+log_Î©ç©ºé—´ç½‘æ ¼æœç´¢
        å¯¹äºæ¯ä¸ªÎ¾ï¼š
        1. ä»ä½åˆ°é«˜æ‰«ælog_Î©ï¼ˆ-13è‡³-1ï¼‰
        2. å¯¹æ¯ä¸ªç‚¹ï¼Œæ¨¡æ‹Ÿè§‚æµ‹å¹¶é‡‡æ ·åéªŒ
        3. æ£€æŸ¥5%åˆ†ä½æ•°æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        ã€æ›¿ä»£æ–¹æ³•ã€‘
        - è´å¶æ–¯å‡è®¾æ£€éªŒï¼ˆBF>10ï¼‰ï¼šè®¡ç®—æˆæœ¬é«˜ï¼Œéœ€åµŒå¥—é‡‡æ ·
        - Fisherä¿¡æ¯çŸ©é˜µï¼šä»…é€‚ç”¨äºé«˜æ–¯ä¼¼ç„¶ï¼Œæ­¤å¤„æ— æ•ˆ
        - æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨ï¼šç›´æ¥è¾“å‡º"å¯æ¢æµ‹/ä¸å¯æ¢æµ‹"ï¼Œä½†æŸå¤±å‚æ•°ä¼°è®¡ä¿¡æ¯
        ã€å½“å‰æ–¹æ³•ä¼˜åŠ¿ã€‘ç›´æ¥å…³è”åˆ°åéªŒç½®ä¿¡åŒºé—´ï¼Œç¬¦åˆè´å¶æ–¯å†³ç­–ç†è®ºã€‚
        """

        
        xi_vals = [0.001, 0.01, 0.1, 0.5, 1.0]  # å®šä¹‰è¦è¯„ä¼°çš„å ç©ºæ¯”åˆ—è¡¨ï¼ˆæ¶µç›–ä»ç¨€ç–çˆ†å‘åˆ°è¿ç»­èƒŒæ™¯ï¼‰
        lim_ai = []  # å­˜å‚¨ ING-Net (AI) åœ¨ä¸åŒå ç©ºæ¯”ä¸‹çš„çµæ•åº¦æé™
        lim_tr = []  # å­˜å‚¨ Traditional (ä¼ ç»Ÿæ–¹æ³•) çš„çµæ•åº¦æé™
        
        for xi in tqdm(xi_vals, desc="è¯„ä¼°"):
            ai_vals = []  # ä¸´æ—¶å­˜å‚¨å½“å‰ xi ä¸‹ï¼ŒAI æ¨¡å‹çš„å¤šæ¬¡å®éªŒç»“æœ
            tr_vals = []  # ä¸´æ—¶å­˜å‚¨å½“å‰ xi ä¸‹ï¼Œä¼ ç»Ÿæ¨¡å‹çš„å¤šæ¬¡å®éªŒç»“æœ
            
            # ã€è’™ç‰¹å¡æ´›å¹³å‡ã€‘
            # å¯¹æ¯ä¸ªå ç©ºæ¯”é‡å¤å®éªŒ 3 æ¬¡ï¼Œå–å¹³å‡å€¼ä»¥å‡å°‘å•æ¬¡å™ªå£°å®ç°çš„éšæœºæ³¢åŠ¨å½±å“
            for _ in range(3):
                # é¢„å®šä¹‰æµ‹è¯•å‚æ•°ï¼ˆå®é™…å€¼ä¼šåœ¨å†…éƒ¨å¾ªç¯çš„ omega_scan ä¸­è¢«è¦†ç›–æ›´æ–°ï¼‰
                test_theta = torch.tensor([[-8.0, xi]], device=device)
                obs = simulator(test_theta).squeeze(0).to(device)
                
                # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ¨¡æ‹Ÿå™¨ç”Ÿæˆ NaN (éæ•°å€¼) å¯¼è‡´ç¨‹åºå´©æºƒ
                if torch.isnan(obs).any():
                    continue
                
                # ã€çº¿æ€§æ‰«æç­–ç•¥ã€‘
                # ä»æå¼±ä¿¡å· (-13) åˆ°å¼ºä¿¡å· (-1) ç”Ÿæˆ 60 ä¸ªç½‘æ ¼ç‚¹
                # ç›®çš„ï¼šå¯»æ‰¾"åˆšåˆšå¥½"èƒ½è¢«æ¢æµ‹åˆ°çš„é‚£ä¸ªèƒ½é‡é˜ˆå€¼
                omega_scan = torch.linspace(-13.0, -1.0, 60, device=device)
                found = False  # æ ‡è®°æ˜¯å¦æ‰¾åˆ°æ¢æµ‹é˜ˆå€¼
                
                for log_omega in omega_scan:
                    # 1. ã€æ³¨å…¥ä¿¡å·ã€‘ï¼šæ„å»ºå½“å‰æ‰«æå¼ºåº¦ log_omega å’Œå›ºå®šå ç©ºæ¯” xi çš„å‚æ•°
                    test_theta = torch.tensor([[log_omega.item(), xi]], device=device)
                    # 2. ã€ç”Ÿæˆè§‚æµ‹ã€‘ï¼šè°ƒç”¨æ¨¡æ‹Ÿå™¨ç”Ÿæˆå«å™ªå£°çš„è§‚æµ‹æ•°æ®
                    obs = simulator(test_theta).squeeze(0).to(device)
                    
                    if torch.isnan(obs).any():
                        continue
                    
                    # 3. ã€åéªŒé‡‡æ ·ã€‘ï¼šæ ¹æ®ç‰¹å¾ç»´åº¦å†³å®šä½¿ç”¨å“ªä¸ªæ¨¡å‹è¿›è¡Œæ¨æ–­
                    # å¦‚æœæ˜¯ 4 ç»´ç‰¹å¾ (CC, K_H, K_L, PW)ï¼Œä½¿ç”¨ AI æ¨¡å‹
                    if obs.shape[0] == 4:
                        samps = post_ai.sample((500,), x=obs, show_progress_bars=False)
                    # å¦åˆ™ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹ (éœ€æ‰‹åŠ¨åˆ‡ç‰‡ï¼Œåªå– CC å’Œ PW)
                    else:
                        samps = post_tr.sample((500,), x=obs[[0, 3]], show_progress_bars=False)
                    
                    # 4. ã€åˆ¤å®šå‡†åˆ™ã€‘ï¼šæ£€æŸ¥åéªŒåˆ†å¸ƒçš„ç½®ä¿¡åŒºé—´ä¸‹ç•Œ
                    # torch.quantile(..., 0.05): è®¡ç®—é¢„æµ‹èƒ½é‡åˆ†å¸ƒçš„ 5% åˆ†ä½æ•°
                    # > -11.0: è¿™æ˜¯ä¸€ä¸ªç»éªŒé˜ˆå€¼ã€‚å¦‚æœ 95% çš„æ¦‚ç‡è®¤ä¸ºèƒ½é‡ > -11 (èƒŒæ™¯å™ªå£°åº•)ï¼Œ
                    #          åˆ™è®¤ä¸ºè¯¥ä¿¡å·å·²è¢«"ç¡®ä¿¡"åœ°æ¢æµ‹åˆ°ã€‚
                    if torch.quantile(samps[:, 0], 0.05).item() > -11.0:
                        # è®°å½•å½“å‰èƒ½é‡å€¼ log_omegaï¼Œå¹¶æ ¹æ®ç»´åº¦å­˜å…¥å¯¹åº”åˆ—è¡¨
                        ai_vals.append(log_omega.item()) if obs.shape[0] == 4 else tr_vals.append(log_omega.item())
                        found = True
                        break  # ã€æ—©åœæœºåˆ¶ã€‘ï¼šä¸€æ—¦æ¢æµ‹åˆ°ï¼Œåç»­æ›´å¼ºçš„ä¿¡å·è‚¯å®šä¹Ÿèƒ½æ¢æµ‹åˆ°ï¼Œæ— éœ€ç»§ç»­æ‰«æ
                
                # å¦‚æœæ‰«æå®Œæ‰€æœ‰å¼ºåº¦éƒ½æœªèƒ½è§¦å‘æ¢æµ‹é˜ˆå€¼ï¼Œæ ‡è®°ä¸º -1.0 (æœªæ£€æµ‹)
                if not found:
                    ai_vals.append(-1.0) if obs.shape[0] == 4 else tr_vals.append(-1.0)
            
            # è®¡ç®— 3 æ¬¡è’™ç‰¹å¡æ´›å®éªŒçš„å¹³å‡å€¼ï¼Œä½œä¸ºè¯¥å ç©ºæ¯”ä¸‹çš„æœ€ç»ˆçµæ•åº¦
            lim_ai.append(np.mean(ai_vals) if ai_vals else -1.0)
            lim_tr.append(np.mean(tr_vals) if tr_vals else -1.0)
        
        # ä¿å­˜ç»“æœ
        group_key = group['name']
        CURVE_DATA[group_key] = {
            "xi": xi_vals,
            "ai": lim_ai,
            "trad": lim_tr,
            "scaling_factor": scaling_factor,
            "dataset": dataset,
            "h1_file": os.path.basename(group['h1']),
            "l1_file": os.path.basename(group['l1'])
        }
        
        torch.save(CURVE_DATA, SAVE_FILE)
        print(f"   ğŸ’¾ å·²ä¿å­˜æ•°æ®ç»„ç»“æœ")
        
        # ==========================================
        # å¯è§†åŒ–ï¼šå•æ•°æ®ç»„åˆ†æ
        # ==========================================
        print("   ğŸ¨ ç”Ÿæˆæ•°æ®ç»„å›¾è¡¨...")
        res = CURVE_DATA[group_key]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))
        
        # ä¸Šå›¾ï¼šçµæ•åº¦æ›²çº¿ Î©(Î¾)
        ax1.plot(res['xi'], res['ai'], 'o-', color='#1f77b4', lw=2.5, markersize=8, label='ING-Net (AI)')
        ax1.plot(res['xi'], res['trad'], 's--', color='#ff7f0e', lw=2.5, markersize=8, label='Traditional')
        ax1.set_xscale('log')
        ax1.set_ylim(-14.0, -1.0)
        ax1.set_xlabel(r"Duty Cycle $\xi$")
        ax1.set_ylabel(r"$\log_{10}\Omega_{gw}$")
        ax1.set_title(f"{dataset} Group: {group['name']}\n{res['h1_file'][:25]}...")
        ax1.grid(True, alpha=0.3, which="both")
        ax1.legend()
        
        # ä¸‹å›¾ï¼šSNRåˆ†æ
        """
        ã€ä¿¡å™ªæ¯”è®¡ç®—ã€‘
        
        å•çˆ†å‘SNRå…¬å¼ï¼š
        SNR = sqrt(Î© / Î¾) * scaling_factor
        
        æ¨å¯¼ï¼šç”± amp = sqrt(Î©/Î¾)*scaling_factorï¼Œä¸”SNRâˆampï¼ˆé«˜æ–¯å™ªå£°èƒŒæ™¯ï¼‰
        
        å‚è€ƒçº¿ï¼š
        - SNR=8ï¼šé«˜ç½®ä¿¡åº¦æ¢æµ‹é˜ˆå€¼ï¼ˆå¯¹åº”FAR<1/centuryï¼‰
        - SNR=5ï¼šå€™é€‰äº‹ä»¶é˜ˆå€¼
        
        æ›²çº¿æ˜¾ç¤ºåœ¨çµæ•åº¦æé™å¤„ï¼Œå•æ¬¡çˆ†å‘çš„å…¸å‹SNRï¼Œ
        éªŒè¯æ–¹æ³•è‡ªæ´½æ€§ï¼ˆé€šå¸¸åº”>5ä»¥ä¿è¯ç´¯ç§¯æ¢æµ‹ï¼‰ã€‚
        """
        # ä»ä¹‹å‰çš„åˆ†æç»“æœ res ä¸­æå– AI æ¨¡å‹çš„æ¢æµ‹æé™ (log_omega å€¼)
        ai_limit = res['ai'][0]
        # åªæœ‰å½“æˆåŠŸæ‰¾åˆ°æ¢æµ‹æé™æ—¶ï¼ˆä¸ç­‰äº -1.0ï¼‰ï¼Œæ‰ç»˜åˆ¶ SNR åˆ†æå›¾
        if ai_limit != -1.0:
            # è·å–è¯¥æ•°æ®é›†å¯¹åº”çš„ç‰©ç†æ ‡åº¦å› å­ (ä¾‹å¦‚ O3a=1300, O4a=1200)
            scaling_factor = res['scaling_factor']
            # ã€ç”Ÿæˆç»˜å›¾æ•°æ®ç‚¹ã€‘
            # åœ¨æ¢æµ‹æé™ ai_limit çš„å·¦å³å„æ‰©å±• 1.5 ä¸ªæ•°é‡çº§ï¼Œç”Ÿæˆ 30 ä¸ªé‡‡æ ·ç‚¹
            # ç›®çš„ï¼šå±•ç¤ºåœ¨æ¢æµ‹é˜ˆå€¼é™„è¿‘çš„ SNR å˜åŒ–è¶‹åŠ¿
            omega_vals = np.linspace(ai_limit - 1.5, ai_limit + 1.5, 30)
            # ã€ç‰©ç†å…¬å¼è½¬æ¢ï¼šèƒ½é‡ -> SNRã€‘
            # è®¡ç®—å¯¹åº”çš„å•æ¬¡çˆ†å‘ä¿¡å™ªæ¯” (Single Burst SNR)ã€‚
            # å…¬å¼åŸç†ï¼š
            #   1. 10**o: å°†å¯¹æ•°èƒ½é‡ log_omega è¿˜åŸä¸ºçº¿æ€§èƒ½é‡ Î©ã€‚
            #   2. / 0.001: å½’ä¸€åŒ–å¤„ç†ï¼ˆå‡è®¾å‚è€ƒå ç©ºæ¯”æˆ–æŒç»­æ—¶é—´ï¼‰ã€‚
            #   3. sqrt(...): æŒ¯å¹…ä¸èƒ½é‡çš„å¹³æ–¹æ ¹æˆæ­£æ¯”ã€‚
            #   4. * scaling_factor: ä¹˜ä»¥å‰é¢çš„æ ‡åº¦å› å­ï¼Œå¾—åˆ°æ¢æµ‹å™¨è¾“å‡ºçš„å®é™… SNRã€‚
            snr_vals = [np.sqrt(10**o / 0.001) * scaling_factor for o in omega_vals]
            # ã€ç»˜åˆ¶ SNR æ›²çº¿ã€‘
            # ç»˜åˆ¶ èƒ½é‡(Xè½´) vs ä¿¡å™ªæ¯”(Yè½´) çš„å…³ç³»å›¾
            # 'o-': å¸¦åœ†ç‚¹çš„å®çº¿; color='purple': ç´«è‰²; markersize=3: ç‚¹çš„å¤§å°
            ax2.plot(omega_vals, snr_vals, 'o-', color='purple', linewidth=2, markersize=3)
            # ã€ç»˜åˆ¶å‚è€ƒçº¿ã€‘
            # åˆ’çº¢çº¿ï¼šSNR = 8.0ã€‚è¿™æ˜¯å¼•åŠ›æ³¢æ¢æµ‹é¢†åŸŸçš„"é»„é‡‘æ ‡å‡†"é˜ˆå€¼ï¼Œé€šå¸¸è®¤ä¸º SNR>8 æ‰æ˜¯å¯ä¿¡ä¿¡å·ã€‚
            ax2.axhline(y=8.0, color='r', linestyle='--', alpha=0.7, label='SNR~8 (æ ‡å‡†é˜ˆå€¼)')
            # åˆ’ç»¿çº¿ï¼šSNR = 5.0ã€‚è¿™æ˜¯è¾ƒå¼±ä¿¡å·çš„å‚è€ƒçº¿ï¼Œè¡¨ç¤º"å‹‰å¼ºå¯è§"ã€‚
            ax2.axhline(y=5.0, color='g', linestyle='--', alpha=0.7, label='SNR~5 (å¼±ä¿¡å·)')
            # ã€ç»˜åˆ¶æ¢æµ‹æé™çº¿ã€‘
            # åœ¨ X è½´ä¸Šæ ‡å‡º AI å®é™…æµ‹å¾—çš„æé™ä½ç½®ã€‚
            # è¿™æ¡çº¿ä¸ç´«è‰²æ›²çº¿çš„äº¤ç‚¹ï¼Œå¦‚æœè½åœ¨ SNR=5~8 ä¹‹é—´ï¼Œè¯´æ˜ AI çš„æ€§èƒ½ç¬¦åˆé¢„æœŸã€‚
            ax2.axvline(x=ai_limit, color='b', linestyle='-.', alpha=0.8, label=f'Limit={ai_limit:.2f}')
            # è®¾ç½® Y è½´ä¸ºå¯¹æ•°åæ ‡ï¼Œå› ä¸º SNR çš„è·¨åº¦å¯èƒ½å¾ˆå¤§
            ax2.set_yscale('log')
            # æ·»åŠ ç½‘æ ¼çº¿ï¼Œalpha=0.3 è®¾ç½®é€æ˜åº¦
            ax2.grid(True, alpha=0.3)
            # æ˜¾ç¤ºå›¾ä¾‹ï¼Œå­—ä½“è®¾å°ä»¥å…é®æŒ¡
            ax2.legend(fontsize=7)
        # -------------------------------------------------------
        # è®¾ç½®å›¾è¡¨æ ‡ç­¾ä¸æ ‡é¢˜
        # -------------------------------------------------------
        # ä½¿ç”¨ LaTeX æ ¼å¼æ˜¾ç¤ºæ•°å­¦ç¬¦å· Omega (Î©)
        ax2.set_xlabel(r'$\log_{10}\Omega$')
        ax2.set_ylabel('Single Burst SNR')
        # æ ‡é¢˜ä¸­æ³¨æ˜å½“å‰çš„æ•°æ®é›† (Dataset) å’Œä½¿ç”¨çš„æ ‡åº¦å› å­ (SF)
        ax2.set_title(f'{dataset} Group SNR Analysis (SF={res["scaling_factor"]:.0f})')
        # è®¾ç½®æ•´å¼ å¤§å›¾çš„æ€»æ ‡é¢˜ (Super Title)
        plt.suptitle(f"Phase 9: {dataset} Group Analysis", fontsize=14, fontweight='bold')
        # è‡ªåŠ¨è°ƒæ•´å­å›¾é—´è·ï¼Œé˜²æ­¢æ ‡ç­¾é‡å 
        plt.tight_layout()
        # -------------------------------------------------------
        # ä¿å­˜å›¾è¡¨åˆ°ç¡¬ç›˜
        # -------------------------------------------------------
        # ç”Ÿæˆæ—¶é—´æˆ³ï¼Œæ ¼å¼å¦‚ï¼š20231027_153000
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        # æ‹¼æ¥æ–‡ä»¶åï¼šdataset_groupå_æ—¶é—´æˆ³.png
        # ç¤ºä¾‹ï¼šo3a_group_chunk1_20231027_153000.png
        chart_filename = f"{dataset.lower()}_group_{group['name']}_{timestamp}.png"
        # æ‹¼æ¥å®Œæ•´è·¯å¾„ (CACHE_DIR æ˜¯ä¹‹å‰å®šä¹‰çš„ç¼“å­˜ç›®å½•)
        chart_path = os.path.join(CACHE_DIR, chart_filename)
        # ä¿å­˜å›¾ç‰‡ï¼Œdpi=150 ä¿è¯æ¸…æ™°åº¦é€‚ä¸­
        plt.savefig(chart_path, dpi=150)
        # å…³é—­å½“å‰å›¾å½¢ä¸Šä¸‹æ–‡ï¼Œé‡Šæ”¾å†…å­˜ (åœ¨å¾ªç¯ç»˜å›¾ä¸­éå¸¸é‡è¦ï¼Œå¦åˆ™å†…å­˜ä¼šçˆ†)
        plt.close()
        print(f"   ğŸ“Š å›¾è¡¨å·²ä¿å­˜: {chart_path}")
        # èµ„æºé‡Šæ”¾
        del simulator, gpu_segments, post_ai, post_tr, h1_gpu, l1_gpu
        torch.cuda.empty_cache()
    print(f"\n{'='*60}")
    print(f"âœ… {dataset} æ•°æ®é›†æ‰€æœ‰æ•°æ®ç»„å¤„ç†å®Œæˆ!")
    print(f"{'='*60}")

# ==========================================
# 7. æ±‡æ€»åˆ†æä¸å¯è§†åŒ–
# ==========================================
print(f"\n{'='*60}")
print("ğŸ¨ ç”Ÿæˆæ±‡æ€»å›¾è¡¨...")
print(f"{'='*60}")

if len(CURVE_DATA) == 0:
    raise RuntimeError("æ²¡æœ‰æ•°æ®å¯ç»˜å›¾")

# æŒ‰æ•°æ®é›†åˆ†ç»„
dataset_groups = defaultdict(list)
for group_key, data in CURVE_DATA.items():
    dataset = data.get('dataset', 'Unknown')
    dataset_groups[dataset].append((group_key, data))

n_datasets = len(dataset_groups)
if n_datasets == 0:
    raise RuntimeError("æ²¡æœ‰æ•°æ®å¯ç»˜å›¾")

# ä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆæ±‡æ€»å›¾è¡¨
for dataset, groups in dataset_groups.items():
    print(f"\nğŸ“Š ç”Ÿæˆ {dataset} æ•°æ®é›†æ±‡æ€»å›¾è¡¨...")
    
    # è®¡ç®—ç®—æœ¯å¹³å‡æ€§èƒ½
    avg_ai = np.zeros(len(groups[0][1]['xi']))
    avg_trad = np.zeros(len(groups[0][1]['xi']))
    
    for group_key, data in groups:
        avg_ai += np.array(data['ai'])
        avg_trad += np.array(data['trad'])
    
    avg_ai /= len(groups)
    avg_trad /= len(groups)
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))
    
    # çµæ•åº¦æ›²çº¿
    ax1.plot(groups[0][1]['xi'], avg_ai, 'o-', color='#1f77b4', lw=2.5, markersize=8, label='ING-Net (AI)')
    ax1.plot(groups[0][1]['xi'], avg_trad, 's--', color='#ff7f0e', lw=2.5, markersize=8, label='Traditional')
    ax1.set_xscale('log')
    ax1.set_ylim(-14.0, -1.0)
    ax1.set_xlabel(r"Duty Cycle $\xi$")
    ax1.set_ylabel(r"$\log_{10}\Omega_{gw}$")
    ax1.set_title(f"{dataset} æ•°æ®é›†å¹³å‡çµæ•åº¦æ›²çº¿\n({len(groups)} ä¸ªæ•°æ®ç»„)", fontweight='bold')
    ax1.grid(True, alpha=0.3, which="both")
    ax1.legend()
    
    # SNRåˆ†æ
    ai_limit = avg_ai[0]
    if ai_limit != -1.0:
        scaling_factor = groups[0][1]['scaling_factor']
        omega_vals = np.linspace(ai_limit - 1.5, ai_limit + 1.5, 30)
        snr_vals = [np.sqrt(10**o / 0.001) * scaling_factor for o in omega_vals]
        
        ax2.plot(omega_vals, snr_vals, 'o-', color='purple', linewidth=2, markersize=3)
        ax2.axhline(y=8.0, color='r', linestyle='--', alpha=0.7, label='SNR~8')
        ax2.axhline(y=5.0, color='g', linestyle='--', alpha=0.7, label='SNR~5')
        ax2.axvline(x=ai_limit, color='b', linestyle='-.', alpha=0.8, label=f'Avg Limit={ai_limit:.2f}')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
        ax2.legend(fontsize=7)
    
    ax2.set_xlabel(r'$\log_{10}\Omega$')
    ax2.set_ylabel('Single Burst SNR')
    ax2.set_title(f'{dataset} æ•°æ®é›† SNR åˆ†æ (SF={groups[0][1]["scaling_factor"]:.0f})')
    
    plt.suptitle(f"Phase 9: {dataset} Dataset Analysis", fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    plt.savefig(os.path.join(CACHE_DIR, f"{dataset.lower()}_summary_{timestamp}.png"), dpi=150)
    plt.close()
    print(f"   âœ… {dataset} æ±‡æ€»å›¾è¡¨å·²ä¿å­˜")

# ç”Ÿæˆè·¨æ•°æ®é›†å¯¹æ¯”å›¾
print(f"\nğŸ“Š ç”Ÿæˆç»¼åˆåˆ†æå›¾è¡¨...")
fig, axes = plt.subplots(2, n_datasets, figsize=(6*n_datasets, 12))

if n_datasets == 1:
    axes = axes.reshape(2, 1)

for i, (dataset, groups) in enumerate(dataset_groups.items()):
    # è®¡ç®—å¹³å‡
    avg_ai = np.zeros(len(groups[0][1]['xi']))
    avg_trad = np.zeros(len(groups[0][1]['xi']))
    
    for group_key, data in groups:
        avg_ai += np.array(data['ai'])
        avg_trad += np.array(data['trad'])
    
    avg_ai /= len(groups)
    avg_trad /= len(groups)
    
    # çµæ•åº¦æ›²çº¿
    ax1 = axes[0, i]
    ax1.plot(groups[0][1]['xi'], avg_ai, 'o-', color='#1f77b4', lw=2.5, label='ING-Net (AI)')
    ax1.plot(groups[0][1]['xi'], avg_trad, 's--', color='#ff7f0e', lw=2.5, label='Traditional')
    ax1.set_xscale('log')
    ax1.set_ylim(-14.0, -1.0)
    ax1.set_title(f"{dataset}\n({len(groups)} groups)", fontweight='bold')
    ax1.grid(True, alpha=0.3, which="both")
    ax1.set_ylabel(r"$\log_{10}\Omega_{gw}$")
    ax1.set_xlabel(r"Duty Cycle $\xi$")
    if i == 0:
        ax1.legend()
    
    # ========================================================================== 
    # 2. SNR éªŒè¯åˆ†æç»˜å›¾ (Subplot Row 2) 
    # ========================================================================== 
    # é€‰æ‹©ç”»å¸ƒçš„ç¬¬äºŒè¡Œ(Row 1)ã€ç¬¬ i åˆ—çš„å­å›¾ã€‚ 
    # è¿™é‡Œçš„ i é€šå¸¸å¯¹åº”ä¸åŒçš„è§‚æµ‹è¿è¡Œé˜¶æ®µï¼ˆå¦‚ 0->O3a, 1->O3b, 2->O4aï¼‰ã€‚ 
    ax2 = axes[1, i] 
    # è·å–è¯¥æ•°æ®é›†ä¸‹æ‰€æœ‰æ•°æ®ç»„çš„å¹³å‡æ¢æµ‹æé™ (ä¹‹å‰è®¡ç®—å¥½çš„ avg_ai)ã€‚ 
    # avg_ai[0] å¯¹åº” log_omega (èƒ½é‡å¼ºåº¦)ã€‚ 
    ai_limit = avg_ai[0] 
    # å¦‚æœæ¢æµ‹æé™æœ‰æ•ˆï¼ˆå³æ‰¾åˆ°äº†çµæ•åº¦é˜ˆå€¼ï¼Œé -1.0ï¼‰ï¼Œåˆ™ç»˜åˆ¶ SNR æ›²çº¿ 
    if ai_limit != -1.0: 
        # è·å–ç‰©ç†æ ‡åº¦å› å­ (Scaling Factor)ã€‚ 
        # ç”±äºåŒä¸€ä¸ªæ•°æ®é›†(dataset)ä¸‹çš„æ‰€æœ‰ç»„(groups)ç‰©ç†é…ç½®ç›¸åŒï¼Œ 
        # æ‰€ä»¥ç›´æ¥å–ç¬¬ä¸€ä¸ªç»„ groups[0] çš„å‚æ•°å³å¯ã€‚ 
        scaling_factor = groups[0][1]['scaling_factor'] 
        # ã€æ„å»ºç»˜å›¾æ•°æ®ï¼šèƒ½é‡ vs SNRã€‘ 
        # 1. ä»¥æ¢æµ‹æé™ ai_limit ä¸ºä¸­å¿ƒï¼Œå·¦å³å„å»¶ä¼¸ 1.5 ä¸ªæ•°é‡çº§ï¼Œç”Ÿæˆ X è½´åæ ‡ã€‚ 
        omega_vals = np.linspace(ai_limit - 1.5, ai_limit + 1.5, 30) 
        # 2. è®¡ç®—å¯¹åº”çš„å•æ¬¡çˆ†å‘ä¿¡å™ªæ¯” (SNR)ã€‚ 
        # ç‰©ç†å«ä¹‰ï¼šéªŒè¯ AI æ‰¾åˆ°çš„è¿™ä¸ª"æé™"åˆ°åº•å¯¹åº”å¤šå¤§çš„ç‰©ç†ä¿¡å·å¼ºåº¦ï¼Ÿ 
        # å…¬å¼ï¼šSNR = sqrt(èƒ½é‡ / å ç©ºæ¯”å½’ä¸€åŒ–å¸¸æ•°) * æ¢æµ‹å™¨æ ‡åº¦ 
        snr_vals = [np.sqrt(10**o / 0.001) * scaling_factor for o in omega_vals] 
        # ã€ç»˜åˆ¶æ›²çº¿ã€‘ 
        # ç”»å‡º SNR éšèƒ½é‡å˜åŒ–çš„æ›²çº¿ï¼ˆç´«è‰²å®çº¿å¸¦åœ†ç‚¹ï¼‰ã€‚ 
        ax2.plot(omega_vals, snr_vals, 'o-', color='purple', linewidth=2, markersize=3) 
        # ã€æ·»åŠ å‚è€ƒé˜ˆå€¼çº¿ã€‘ 
        # çº¢è‰²è™šçº¿ (SNR=8)ï¼šè¿™æ˜¯ç‰©ç†å­¦ç•Œçš„"å‘ç°æ ‡å‡†"ï¼Œå¦‚æœ Limit çº¿äº¤ç‚¹åœ¨æ­¤ä¹‹ä¸Šï¼Œè¯´æ˜ AI æ¯”è¾ƒä¿å®ˆã€‚ 
        ax2.axhline(y=8.0, color='r', linestyle='--', alpha=0.7, label='SNR~8 (æ ‡å‡†)') 
        # ç»¿è‰²è™šçº¿ (SNR=5)ï¼šè¿™æ˜¯"è¾¹ç¼˜ä¿¡å·"æ ‡å‡†ã€‚å¦‚æœ Limit çº¿äº¤ç‚¹åœ¨æ­¤ä¹‹ä¸‹ï¼Œè¯´æ˜ AI éå¸¸æ¿€è¿›ï¼ˆèƒ½åœ¨æä½ä¿¡å™ªæ¯”ä¸‹å·¥ä½œï¼‰ã€‚ 
        ax2.axhline(y=5.0, color='g', linestyle='--', alpha=0.7, label='SNR~5 (å¼±ä¿¡å·)') 
        # ã€æ ‡è®°æ¢æµ‹æé™ã€‘ 
        # è“è‰²ç‚¹åˆ’çº¿ï¼šæ ‡å‡º AI å®é™…æµ‹å¾—çš„èƒ½é‡æé™ä½ç½®ã€‚ 
        ax2.axvline(x=ai_limit, color='b', linestyle='-.', alpha=0.8, label=f'Limit={ai_limit:.2f}')
        # è®¾ç½® Y è½´ä¸ºå¯¹æ•°åˆ»åº¦ï¼ˆå› ä¸º SNR å˜åŒ–èŒƒå›´å¤§ï¼Œä¸”å…³æ³¨ä½æ•°å€¼åŒºåŸŸï¼‰ã€‚ 
        ax2.set_yscale('log') 
        # å¼€å¯ç½‘æ ¼ï¼Œå¢åŠ å¯è¯»æ€§ã€‚ 
        ax2.grid(True, alpha=0.3) 
        # æ˜¾ç¤ºå›¾ä¾‹ï¼Œå­—ä½“ç¼©å°ä»¥èŠ‚çœç©ºé—´ã€‚ 
        ax2.legend(fontsize=7)
    # è®¾ç½®å­å›¾çš„è½´æ ‡ç­¾å’Œæ ‡é¢˜ 
    # Xè½´ï¼šæ³¨å…¥ä¿¡å·çš„å¯¹æ•°èƒ½é‡å¼ºåº¦ (log10 Omega) 
    ax2.set_xlabel(r'$\log_{10}\Omega$') 
    # Yè½´ï¼šå¯¹åº”çš„å•æ¬¡çˆ†å‘ä¿¡å™ªæ¯” 
    ax2.set_ylabel('Single Burst SNR') 
    # æ ‡é¢˜ï¼šæ˜¾ç¤ºå½“å‰æ•°æ®é›†åç§°åŠä½¿ç”¨çš„æ ‡åº¦å› å­ 
    ax2.set_title(f'{dataset} (SF={groups[0][1]["scaling_factor"]:.0f})')
plt.suptitle("Phase 9: GPU Accelerated Group Analysis", fontsize=14, fontweight='bold')
plt.tight_layout()

timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
plt.savefig(os.path.join(CACHE_DIR, f"phase9_gpu_results_{timestamp}.png"), dpi=150)
plt.close()
print(f"âœ… ç»¼åˆåˆ†æå›¾è¡¨å·²ä¿å­˜")

# ==========================================
# 8. ç»“æœæ€»ç»“
# ==========================================
print(f"\n{'='*60}")
print("ğŸ“‹ æœ€ç»ˆæ€»ç»“")
print(f"{'='*60}")
print(f"{'Dataset':<10} | {'Groups':<10} | {'SF':<8} | {'Avg AI@0.001':<15} | {'Avg SNR':<10}")
print("-" * 70)
# ============================================================================== 
# æ±‡æ€»æŠ¥å‘Šç”Ÿæˆ (Final Aggregation & Reporting) 
# ============================================================================== 
# éå†ä¹‹å‰æ”¶é›†å¥½çš„æ‰€æœ‰ç»“æœå­—å…¸ dataset_groupsã€‚ 
# dataset: æ•°æ®é›†åç§° (å¦‚ "O3a", "O4a") 
# groups: è¯¥æ•°æ®é›†ä¸‹åŒ…å«çš„æ‰€æœ‰å­ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª (group_key, data) å…ƒç»„ 
for dataset, groups in dataset_groups.items(): 
    # è·å–è¯¥æ•°æ®é›†ä¸‹åŒ…å«çš„æ–‡ä»¶ç»„æ€»æ•° (ç”¨äºåç»­æ±‚å¹³å‡) 
    total_groups = len(groups) 
    # å¦‚æœè¯¥æ•°æ®é›†æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œç›´æ¥è·³è¿‡ï¼Œé˜²æ­¢é™¤ä»¥é›¶é”™è¯¯ 
    if total_groups == 0: 
        continue 
    # ã€åˆå§‹åŒ–ç´¯åŠ å™¨ã€‘ 
    # åˆ›å»ºå…¨é›¶æ•°ç»„ï¼Œé•¿åº¦ç­‰äºæµ‹è¯•çš„å ç©ºæ¯”(xi)æ•°é‡ã€‚ 
    # æˆ‘ä»¬éšä¾¿å–ç¬¬ä¸€ä¸ªç»„ (groups[0]) çš„æ•°æ®ç»“æ„æ¥ç¡®å®šæ•°ç»„é•¿åº¦ï¼Œå› ä¸ºæ‰€æœ‰ç»„çš„ xi è®¾ç½®æ˜¯ä¸€æ ·çš„ã€‚ 
    # groups[0][1] è®¿é—®çš„æ˜¯ data å­—å…¸ã€‚ 
    avg_ai = np.zeros(len(groups[0][1]['xi']))    # å­˜å‚¨ ING-Net (AI) çš„çµæ•åº¦ç´¯åŠ å’Œ 
    avg_trad = np.zeros(len(groups[0][1]['xi']))  # å­˜å‚¨ Traditional çš„çµæ•åº¦ç´¯åŠ å’Œ 
    # ã€ç´¯åŠ å¾ªç¯ã€‘ 
    # éå†è¯¥æ•°æ®é›†ä¸‹çš„æ¯ä¸€ä¸ªæ•°æ®ç»„ç»“æœï¼Œå°†çµæ•åº¦æ•°å€¼åŠ åˆ°ç´¯åŠ å™¨ä¸Š 
    for group_key, data in groups: 
        avg_ai += np.array(data['ai'])      # ç´¯åŠ  AI æ¨¡å‹çš„ log_omega æé™ 
        avg_trad += np.array(data['trad'])  # ç´¯åŠ  ä¼ ç»Ÿæ¨¡å‹ çš„ log_omega æé™ 
    # ã€è®¡ç®—å¹³å‡å€¼ã€‘ 
    # æ€»å’Œ / æ•°é‡ = å¹³å‡çµæ•åº¦ 
    avg_ai /= total_groups 
    avg_trad /= total_groups 
    # ã€è®¡ç®—å…¸å‹ä¿¡å™ªæ¯” (SNR) æŒ‡æ ‡ã€‘ 
    # å–æœ€ç¨€ç–çš„ä¸€ä¸ªç‚¹ (index 0, å¯¹åº” xi=0.001) çš„æ¢æµ‹æé™ä½œä¸ºä»£è¡¨æ€§æŒ‡æ ‡ã€‚ 
    lim = avg_ai[0] 
    # è·å–è¯¥æ•°æ®é›†ä½¿ç”¨çš„ç‰©ç†æ ‡åº¦å› å­ (ç”¨äºèƒ½é‡->SNRè½¬æ¢) 
    scaling_factor = groups[0][1]['scaling_factor'] 
    # å°†å¯¹æ•°èƒ½é‡æé™ (lim) è½¬æ¢ä¸ºçº¿æ€§ä¿¡å™ªæ¯” (SNR)ï¼Œç”¨äºç›´è§‚è¯„ä¼°ç‰©ç†æ„ä¹‰ã€‚ 
    # å¦‚æœæ¢æµ‹æé™æ˜¯ -1.0 (ä»£è¡¨æ²¡æ¢æµ‹åˆ°)ï¼Œåˆ™ SNR è®°ä¸º 0ã€‚ 
    # å…¬å¼: SNR = sqrt(10^log_omega / 0.001) * scaling_factor 
    snr = np.sqrt(10**lim / 0.001) * scaling_factor if lim != -1.0 else 0 
    # ã€æ‰“å°è¡¨æ ¼è¡Œã€‘ 
    # æ ¼å¼åŒ–è¾“å‡ºç»Ÿè®¡ç»“æœï¼Œå¯¹é½åˆ—å®½ä»¥ä¾¿é˜…è¯»ã€‚ 
    # <10: å·¦å¯¹é½å 10ä½; .2f: ä¿ç•™ä¸¤ä½å°æ•° 
    # è¾“å‡ºå†…å®¹ï¼šæ•°æ®é›†å | ç»„æ•° | æ ‡åº¦å› å­ | å¹³å‡æ¢æµ‹æé™(èƒ½é‡) | å¯¹åº”çš„SNR 
    print(f"{dataset:<10} | {total_groups:<10} | {scaling_factor:<8.0f} | " 
          f"{lim:<15.2f} | {snr:<10.2f}")

print(f"\n{'='*60}")
print("ğŸ“‹ æ•°æ®ç»„è¯¦ç»†ç»Ÿè®¡")
print(f"{'='*60}")
print(f"{'Group':<20} | {'Dataset':<10} | {'SF':<8} | {'AI@0.001':<12} | {'SNR':<10}")
print("-" * 70)

top_groups = []
for group_key, data in CURVE_DATA.items():
    lim = data['ai'][0]
    scaling_factor = data['scaling_factor']
    snr = np.sqrt(10**lim / 0.001) * scaling_factor if lim != -1.0 else 0
    top_groups.append((group_key, data.get('dataset', 'Unknown'), scaling_factor, lim, snr))

top_groups.sort(key=lambda x: x[4], reverse=True)
for group_key, dataset, sf, lim, snr in top_groups[:10]:
    print(f"{group_key[:19]:<20} | {dataset:<10} | {sf:<8.0f} | {lim:<12.2f} | {snr:<10.2f}")

print(f"\nâœ… å®Œæˆ! æ˜¾å­˜ä½¿ç”¨å³°å€¼: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {CACHE_DIR}")
print(f"ğŸ“Š å…±å¤„ç† {len(CURVE_DATA)} ä¸ªæ•°æ®ç»„")