# @title Phase 9 (O3b): Ultra-Fast GPU Edition (With Model Saving)
# @markdown **ğŸš€ Features:**
# @markdown 1. **Pure GPU Acceleration:** Train & Simulate in seconds.
# @markdown 2. **Model Saving:** Automatically saves `ing_net.pt` and `trad_model.pt`.
# @markdown 3. **High Precision:** N=20000 samples for reliable results.

import os
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from sbi.inference import SNPE
from sbi.utils import BoxUniform
from tqdm import tqdm
import warnings
import datetime

warnings.filterwarnings("ignore")

print("=== P903b_GPU_UltraFast_WithSave.py å¯åŠ¨ ===")

# ==================== é…ç½®åŒºåŸŸ ====================
PT_DATA_DIR = r"C:\Users\20466\Desktop\æ–°å»ºæ–‡ä»¶å¤¹ (6)\LIGO_Data_Cache"
CACHE_DIR = r"C:\Users\20466\Desktop\æ–°å»ºæ–‡ä»¶å¤¹ (6)\LIGO_Data_Cache"
XI_TARGET = 0.001
SCALING_FACTOR = 1200.0 
N_TRAIN = 20000   
N_CALIB = 1000    
CUTOFF = 25.0  # âœ… æ–°å¢ï¼šé«˜é€šæ»¤æ³¢æˆªæ­¢é¢‘ç‡ï¼ˆHzï¼‰ï¼ŒO3bå»ºè®®25~35

# æ£€æŸ¥ GPU
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"æ ¸å¿ƒè®¾å¤‡: {torch.cuda.get_device_name(0)}")
else:
    raise RuntimeError("é”™è¯¯: æœªæ£€æµ‹åˆ° GPU! æ­¤è„šæœ¬éœ€è¦ CUDAã€‚")

# ==================== 1. æ•°æ®åŠ è½½ ====================
def load_data_to_gpu(label="O3b"):
    expected_length = int(4096 * 2048.0)
    expected_length = int(4096 * 2048.0)
    filenames = [f"{label}_H1_1260834498_4.pt", f"{label}_L1_1260834498_4.pt", 
                 f"{label}_H1_1260834498_3.pt", f"{label}_L1_1260834498_3.pt"]
    loaded = {}
    for det in ['H1', 'L1']:
        for fname in filenames:
            if det in fname:
                path = os.path.join(PT_DATA_DIR, fname)
                if os.path.exists(path):
                    try:
                        data = torch.load(path, map_location='cpu', weights_only=False)
                        if isinstance(data, np.ndarray): data = torch.from_numpy(data)
                        if not torch.isfinite(data).all(): continue
                        loaded[det] = data.float().to(device)
                        break
                    except: continue
    h1 = loaded.get('H1', torch.randn(expected_length, device=device))
    l1 = loaded.get('L1', torch.randn(expected_length, device=device))
    min_len = min(len(h1), len(l1))
    return h1[:min_len], l1[:min_len]

class Phase9SimulatorGPU:
    def __init__(self, h1_bg, l1_bg, scaling_factor=1300.0, cutoff=30.0):  # âœ… æ–°å¢cutoffå‚æ•°
        self.h1_bg = h1_bg
        self.l1_bg = l1_bg
        self.scaling_factor = scaling_factor
        self.cutoff = cutoff  # âœ… å­˜å‚¨ä¸ºå®ä¾‹å˜é‡
        self.target_fs = 2048.0
        self.seg_len = int(4.0 * self.target_fs)
        self.max_idx = len(h1_bg) - self.seg_len - 1

    # --- [æ–°å¢] ---
    def apply_highpass_filter(self, x):  # âœ… ç§»é™¤cutoffå‚æ•°ï¼Œä½¿ç”¨self.cutoff
        """O3b å¿…é¡»å»é™¤ <cutoff Hz çš„å™ªå£°"""
        n = x.shape[-1]
        freq = torch.fft.rfftfreq(n, d=1/self.target_fs, device=device)
        fft_x = torch.fft.rfft(x, dim=-1)
        mask = (freq > self.cutoff).float()  # âœ… ä½¿ç”¨å®ä¾‹å˜é‡
        return torch.fft.irfft(fft_x * mask, n=n, dim=-1)

    def compute_features_gpu(self, h1, l1):
        # æ³¨æ„ï¼šè¿™é‡Œçš„è¾“å…¥å·²ç»æ˜¯ç»è¿‡æ»¤æ³¢å’Œå½’ä¸€åŒ–çš„äº†
        vx = h1 - h1.mean(dim=1, keepdim=True)
        vy = l1 - l1.mean(dim=1, keepdim=True)
        cost = (vx * vy).sum(dim=1) / (torch.sqrt((vx**2).sum(dim=1)) * torch.sqrt((vy**2).sum(dim=1)) + 1e-8)
        
        def kurtosis_torch(x):
            mean = x.mean(dim=1, keepdim=True)
            diff = x - mean
            m2 = (diff**2).mean(dim=1)
            m4 = (diff**4).mean(dim=1)
            return m4 / (m2**2 + 1e-8) - 3.0
        
        k_h1 = torch.log1p(torch.abs(kurtosis_torch(h1)))
        k_l1 = torch.log1p(torch.abs(kurtosis_torch(l1)))
        
        # log10(var) å¯¹äº O3b æ¥è¯´ï¼Œå¯èƒ½éœ€è¦æ›´é²æ£’çš„åŠŸç‡ä¼°è®¡ï¼Œä½†æš‚æ—¶ä¿æŒåŸæ ·
        pw = torch.log10(h1.var(dim=1) * l1.var(dim=1) + 1e-30)
        return torch.stack([cost, k_h1, k_l1, pw], dim=1)

    def simulate(self, theta_batch):
        batch_size = theta_batch.shape[0]
        theta_batch = theta_batch.to(device)
        log_omega, xi = theta_batch[:, 0], theta_batch[:, 1]
        
        start_indices = torch.randint(0, self.max_idx, (batch_size,), device=device)
        indices = start_indices.unsqueeze(1) + torch.arange(self.seg_len, device=device)
        
        n_h1 = self.h1_bg[indices] 
        n_l1 = self.l1_bg[indices] 
        
        # === ä¿®æ”¹é‡ç‚¹ 1: å…ˆæ»¤æ³¢ ===
        n_h1 = self.apply_highpass_filter(n_h1)
        n_l1 = self.apply_highpass_filter(n_l1)

        # === ä¿®æ”¹é‡ç‚¹ 2: é²æ£’å½’ä¸€åŒ– ===
        # ä½¿ç”¨ IQR (0.75 - 0.25 åˆ†ä½) ä»£æ›¿ std
        def robust_norm(x):
            q75 = torch.nanquantile(x, 0.75, dim=1, keepdim=True)
            q25 = torch.nanquantile(x, 0.25, dim=1, keepdim=True)
            iqr = q75 - q25
            median = torch.nanquantile(x, 0.5, dim=1, keepdim=True)
            return (x - median) / (iqr / 1.349 + 1e-15)

        n_h1 = robust_norm(n_h1)
        n_l1 = robust_norm(n_l1)
        
        mask_sig = (log_omega > -15.0)
        if mask_sig.any():
            omega = 10**log_omega[mask_sig]
            safe_xi = torch.clamp(xi[mask_sig], min=1e-4)
            # Scaling Factor å¯èƒ½éœ€è¦é’ˆå¯¹ O3b å¾®è°ƒï¼Œå¦‚æœ SNR ä¾ç„¶ä½ï¼Œå°è¯•å¢åŠ å®ƒ
            amp = torch.sqrt(omega / safe_xi) * self.scaling_factor 
            
            n_ev = (self.seg_len * safe_xi * 0.2).long()
            n_ev[xi[mask_sig] >= 0.99] = self.seg_len
            
            # ç”Ÿæˆä¿¡å·å™ªå£°
            raw_noise = torch.randn(mask_sig.sum(), self.seg_len, device=device) * amp.unsqueeze(1)
            
            # ä¿¡å·ä¹Ÿéœ€è¦ç»è¿‡åŒæ ·çš„æ»¤æ³¢ï¼è¿™æ˜¯ç‰©ç†ä¸€è‡´æ€§
            # (è™½ç„¶ç™½å™ªå£°è°±æ˜¯å¹³çš„ï¼Œä½†ä¸ºäº†åŒ¹é…èƒŒæ™¯çš„å¤„ç†æ–¹å¼ï¼Œå»ºè®®åŠ ä¸Š)
            raw_noise = self.apply_highpass_filter(raw_noise)

            starts = torch.randint(0, self.seg_len, (len(n_ev),), device=device)
            starts = torch.min(starts, self.seg_len - n_ev)
            
            positions = torch.arange(self.seg_len, device=device).unsqueeze(0)
            time_mask = (positions >= starts.unsqueeze(1)) & (positions < (starts + n_ev).unsqueeze(1))
            
            from scipy.signal.windows import tukey
            window_cpu = torch.from_numpy(tukey(self.seg_len, alpha=0.1)).float().to(device)
            
            n_h1[mask_sig] += raw_noise * time_mask * window_cpu
            n_l1[mask_sig] += raw_noise * time_mask * window_cpu
            
        return self.compute_features_gpu(n_h1, n_l1)


# ==================== 3. è¾…åŠ©å‡½æ•° ====================
def generate_training_data(sim, prior, n_samples):
    batch_size = 1000
    theta_all, x_all = [], []
    print(f"GPUæ­£åœ¨ç”Ÿæˆ {n_samples} æ¡æ¨¡æ‹Ÿæ•°æ®...")
    # æ·»åŠ è¿›åº¦æ¡
    for i in tqdm(range(0, n_samples, batch_size), desc="[DEBUG] ç”Ÿæˆè®­ç»ƒæ•°æ®", leave=True):
        batch_theta = prior.sample((batch_size,)).to(device)
        batch_x = sim.simulate(batch_theta)
        theta_all.append(batch_theta)
        x_all.append(batch_x)
        # æ¯10ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡è¿›åº¦
        if (i // batch_size) % 10 == 0:
            print(f"[DEBUG] å·²ç”Ÿæˆ {i + batch_size} / {n_samples} æ¡æ•°æ®")
    return torch.cat(theta_all), torch.cat(x_all)

def safe_sample(posterior, x, n_samples=200):
    try:
        return posterior.sample((n_samples,), x=x, show_progress_bars=False)
    except:
        return torch.tensor([[10.0, 0.5]] * n_samples, device=device)

def fast_calibrate(posterior, sim, n, feature_indices=None):
    bs = 100
    print(f"[DEBUG] å¼€å§‹CFARæ ¡å‡†ï¼Œn={n}ï¼Œæ‰¹å¤§å°={bs}...")
    theta_noise = torch.tensor([[-20.0, 0.1]] * n, device=device)
    print("[DEBUG] æ­£åœ¨ç”Ÿæˆå™ªå£°è§‚æµ‹æ•°æ®...")
    obs_noise = sim.simulate(theta_noise)
    scores = []
    total_batches = (n + bs - 1) // bs
    
    for batch_idx in tqdm(range(0, n, bs), desc="[DEBUG] CFARæ ¡å‡†è¿›åº¦", leave=True):
        batch = obs_noise[batch_idx:batch_idx+bs]
        if feature_indices: 
            batch = batch[:, feature_indices]
            print(f"[DEBUG] åº”ç”¨ç‰¹å¾ç´¢å¼•: {feature_indices}")
        
        for sample_idx in range(len(batch)):
            s = safe_sample(posterior, batch[sample_idx])
            scores.append(s[:, 0].mean().item())
        
        # æ¯å¤„ç†5ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡è¿›åº¦
        if (batch_idx // bs + 1) % 5 == 0:
            print(f"[DEBUG] å·²å®Œæˆ {batch_idx + bs} / {n} ä¸ªæ ·æœ¬çš„æ ¡å‡†")
    
    print("[DEBUG] è®¡ç®—90ç™¾åˆ†ä½é˜ˆå€¼...")
    return np.percentile(scores, 90)  # FAR=10%ï¼Œé˜ˆå€¼æ›´ä½ï¼ŒSNRé™ä½

def find_limit(posterior, sim, xi_tgt, thresh, feature_indices=None):
    print(f"[DEBUG] å¼€å§‹å¯»æ‰¾æé™å€¼ï¼Œxi_tgt={xi_tgt}ï¼Œé˜ˆå€¼={thresh}...")
    low, high = -12.0, -1.0
    n_trials = 20
    iteration = 0
    
    while (high - low) > 0.2:
        iteration += 1
        mid = (high + low) / 2.0
        print(f"[DEBUG] è¿­ä»£ {iteration}: æµ‹è¯•å€¼={mid:.4f}ï¼Œå½“å‰èŒƒå›´ [{low:.4f}, {high:.4f}]")
        
        theta_test = torch.tensor([[mid, xi_tgt]] * n_trials, device=device)
        print(f"[DEBUG] æ­£åœ¨ç”Ÿæˆ {n_trials} ä¸ªæµ‹è¯•è§‚æµ‹...")
        obs_test = sim.simulate(theta_test)
        if feature_indices: 
            obs_test = obs_test[:, feature_indices]
            print(f"[DEBUG] åº”ç”¨ç‰¹å¾ç´¢å¼•: {feature_indices}")
        
        detected = 0
        for i in range(n_trials):
            s = safe_sample(posterior, obs_test[i])
            if s[:, 0].mean() > thresh: 
                detected += 1
        
        print(f"[DEBUG] æ£€æµ‹åˆ° {detected} / {n_trials} ä¸ªä¿¡å·")
        if detected >= (n_trials / 2): 
            high = mid
            print(f"[DEBUG] é™ä½ä¸Šé™è‡³ {high:.4f}")
        else: 
            low = mid
            print(f"[DEBUG] æé«˜ä¸‹é™è‡³ {low:.4f}")
    
    print(f"[DEBUG] å¯»æ‰¾æé™å€¼å®Œæˆï¼Œç»“æœ={high:.4f}")
    return high

# ==================== ä¸»æµç¨‹ ====================
if __name__ == "__main__":
    # é‡å¤è¿è¡Œ5æ¬¡
    for run in range(1, 6):
        print(f"\n" + "="*100)
        print(f"==================== ç¬¬ {run} æ¬¡è¿è¡Œ ====================")
        print("="*100)
        print(f"[DEBUG] å¼€å§‹æ‰§è¡Œç¬¬ {run} æ¬¡è¿è¡Œ...")
        
        # 1. æ•°æ®åŠ è½½
        print("[DEBUG] æ­£åœ¨åŠ è½½æ•°æ®...")
        h1_gpu, l1_gpu = load_data_to_gpu("O3b")
        print(f"[DEBUG] æ•°æ®åŠ è½½å®Œæˆï¼ŒH1æ•°æ®é•¿åº¦: {len(h1_gpu)}, L1æ•°æ®é•¿åº¦: {len(l1_gpu)}")
        
        # 2. âœ… ä¿®æ”¹ï¼šåˆå§‹åŒ–æ¨¡æ‹Ÿå™¨æ—¶ä¼ å…¥cutoffå‚æ•°
        print("[DEBUG] æ­£åœ¨åˆå§‹åŒ–æ¨¡æ‹Ÿå™¨...")
        sim_gpu = Phase9SimulatorGPU(h1_gpu, l1_gpu, scaling_factor=SCALING_FACTOR, cutoff=CUTOFF)
        print(f"[DEBUG] æ¨¡æ‹Ÿå™¨åˆå§‹åŒ–å®Œæˆï¼Œcutoff={CUTOFF}Hz")
        
        # 3. è®¾ç½®å…ˆéªŒåˆ†å¸ƒ
        print("[DEBUG] æ­£åœ¨è®¾ç½®å…ˆéªŒåˆ†å¸ƒ...")
        prior = BoxUniform(low=torch.tensor([-13.0, 0.001], device=device), 
                           high=torch.tensor([5.0, 1.0], device=device))
        print("[DEBUG] å…ˆéªŒåˆ†å¸ƒè®¾ç½®å®Œæˆ")
        
        # 4. ç”Ÿæˆè®­ç»ƒæ•°æ®
        print("[DEBUG] æ­£åœ¨ç”Ÿæˆè®­ç»ƒæ•°æ®...")
        theta_tr, x_tr = generate_training_data(sim_gpu, prior, N_TRAIN)
        print(f"[DEBUG] è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆï¼Œæ ·æœ¬æ•°: {len(theta_tr)}")
        
        # 5. è®­ç»ƒING-Netæ¨¡å‹
        print("[DEBUG] æ­£åœ¨è®­ç»ƒING-Netæ¨¡å‹...")
        inf_ai = SNPE(prior=prior, density_estimator="maf", device=str(device))
        inf_ai.append_simulations(theta_tr, x_tr)
        print("[DEBUG] æ­£åœ¨æ‰§è¡ŒING-Netè®­ç»ƒ...")
        post_ai = inf_ai.build_posterior(inf_ai.train(show_train_summary=False))
        print("[DEBUG] ING-Netæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # 6. è®­ç»ƒTraditionalæ¨¡å‹
        print("[DEBUG] æ­£åœ¨è®­ç»ƒTraditionalæ¨¡å‹...")
        inf_tr = SNPE(prior=prior, density_estimator="maf", device=str(device))
        inf_tr.append_simulations(theta_tr, x_tr[:, [0, 3]])
        print("[DEBUG] æ­£åœ¨æ‰§è¡ŒTraditionalè®­ç»ƒ...")
        post_tr = inf_tr.build_posterior(inf_tr.train(show_train_summary=False))
        print("[DEBUG] Traditionalæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # 7. ä¿å­˜æ¨¡å‹
        print("\n[DEBUG] æ­£åœ¨ä¿å­˜æ¨¡å‹æ–‡ä»¶...")
        model_dir = os.path.join(CACHE_DIR, "models")
        os.makedirs(model_dir, exist_ok=True)
        
        # æ·»åŠ æ—¶é—´æˆ³ï¼ˆç²¾ç¡®åˆ°æ¯«ç§’ï¼‰ï¼Œé˜²æ­¢è¦†ç›–åŸæœ‰æ–‡ä»¶
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        
        path_ai = os.path.join(model_dir, f"ing_net_o3b_gpu_{timestamp}.pt")
        path_tr = os.path.join(model_dir, f"trad_model_o3b_gpu_{timestamp}.pt")
        
        print(f"[DEBUG] æ­£åœ¨ä¿å­˜ING-Netæ¨¡å‹è‡³: {path_ai}")
        torch.save(post_ai, path_ai)
        print(f"[DEBUG] æ­£åœ¨ä¿å­˜Traditionalæ¨¡å‹è‡³: {path_tr}")
        torch.save(post_tr, path_tr)
        print(f"[DEBUG] æ¨¡å‹å·²ä¿å­˜è‡³: {model_dir}")
        print(f"[DEBUG] - ING-Net: ing_net_o3b_gpu_{timestamp}.pt")
        print(f"[DEBUG] - Traditional: trad_model_o3b_gpu_{timestamp}.pt")
        
        # 8. CFARæ ¡å‡†
        print(f"\n[DEBUG] å¿«é€ŸCFARæ ¡å‡† (N={N_CALIB})...")
        print("[DEBUG] æ­£åœ¨æ ¡å‡†ING-Neté˜ˆå€¼...")
        thresh_ai = fast_calibrate(post_ai, sim_gpu, N_CALIB, None)
        print(f"[DEBUG] ING-Neté˜ˆå€¼æ ¡å‡†å®Œæˆ: {thresh_ai:.4f}")
        
        print("[DEBUG] æ­£åœ¨æ ¡å‡†Traditionalé˜ˆå€¼...")
        thresh_tr = fast_calibrate(post_tr, sim_gpu, N_CALIB, [0, 3])
        print(f"[DEBUG] Traditionalé˜ˆå€¼æ ¡å‡†å®Œæˆ: {thresh_tr:.4f}")
        
        print(f"[DEBUG] é˜ˆå€¼æ ¡å‡†ç»“æœ: ING-Net={thresh_ai:.4f} | Traditional={thresh_tr:.4f}")
        
        # 9. çµæ•åº¦æ‰«æ
        print("\n[DEBUG] å¼€å§‹æ‰«æçµæ•åº¦...")
        xi_vals = [0.001, 0.01, 0.1, 0.5, 1.0]
        print(f"{'Xi':<6} | {'AI Limit':<10} | {'Trad Limit':<10} | {'Advantage'}")
        print("-" * 55)
        
        res_ai, res_tr = [], []
        for xi in tqdm(xi_vals, desc="[DEBUG] çµæ•åº¦æ‰«æè¿›åº¦"):
            print(f"[DEBUG] æ­£åœ¨å¤„ç†Xi={xi}...")
            
            print(f"[DEBUG] æ­£åœ¨è®¡ç®—ING-Netæé™å€¼...")
            l_ai = find_limit(post_ai, sim_gpu, xi, thresh_ai, None)
            print(f"[DEBUG] ING-Netæé™å€¼è®¡ç®—å®Œæˆ: {l_ai:.2f}")
            
            print(f"[DEBUG] æ­£åœ¨è®¡ç®—Traditionalæé™å€¼...")
            l_tr = find_limit(post_tr, sim_gpu, xi, thresh_tr, [0, 3])
            print(f"[DEBUG] Traditionalæé™å€¼è®¡ç®—å®Œæˆ: {l_tr:.2f}")
            
            res_ai.append(l_ai)
            res_tr.append(l_tr)
            diff = l_tr - l_ai
            adv = "AI Win" if l_ai < l_tr else "Trad Win"
            print(f"{xi:<6} | {l_ai:<10.2f} | {l_tr:<10.2f} | {diff:+.2f} ({adv})")
        
        # 10. ä¿å­˜ç»“æœ
        print(f"\n[DEBUG] æ­£åœ¨ä¿å­˜ç»“æœæ–‡ä»¶...")
        results_path = os.path.join(CACHE_DIR, f"o3b_gpu_results_{timestamp}.pt")
        torch.save({"xi": xi_vals, "ai": res_ai, "trad": res_tr}, results_path)
        print(f"[DEBUG] ç»“æœæ–‡ä»¶ä¿å­˜å®Œæˆ: {results_path}")
        
        print(f"\n[DEBUG] ç¬¬ {run} æ¬¡è¿è¡Œå®Œæˆï¼ç»“æœä¸æ¨¡å‹å‡å·²ä¿å­˜ã€‚")
        print(f"[DEBUG] - ç»“æœæ–‡ä»¶: o3b_gpu_results_{timestamp}.pt")
        print(f"[DEBUG] - ING-Netæ¨¡å‹: ing_net_o3b_gpu_{timestamp}.pt")
        print(f"[DEBUG] - Traditionalæ¨¡å‹: trad_model_o3b_gpu_{timestamp}.pt")
    
    print(f"\n" + "="*100)
    print("==================== æ‰€æœ‰è¿è¡Œå®Œæˆ ====================")
    print("="*100)
    print("[DEBUG] 5æ¬¡è¿è¡Œå·²å…¨éƒ¨å®Œæˆï¼æ‰€æœ‰ç»“æœä¸æ¨¡å‹å‡å·²ä¿å­˜ã€‚")